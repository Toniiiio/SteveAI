yes = "NULL",
no = x
)
)
msg <- do.call(
what = sprintf,
args = c(msg, parsed)
)
}
# avoid line breaks and usage of reserved characters
msg <- gsub(
x = msg,
pattern = paste0(c("\n", trennzeichen), collapse = "|"),
replacement = ""
)
c(time, processID, func, names(level), paste0(msg, "\n")) %>%
paste(collapse = trennzeichen)
}
flog.layout(layoutFunc, name = logger_name)
log_status <- function(status, name, url){
msg <- glue::glue("Server response status code for comp_name:'{name}' is code:{status} for url:'{url}'.")
if(status == 200){
flog.info(
msg = msg,
name = logger_name
)
}else if(status >= 400 & status <= 499){
flog.error(
msg = msg,
name = logger_name
)
}else{
flog.warn(
msg = msg,
name = logger_name
)
}
}
log_node_len <- function(nodes, name, scraper, content, iterNr = 1){
nodes_len <- nodes %>% length
url <- scraper$url
if(!is.null(scraper$no_job_id)){
valid_no_items <- grepl(pattern = scraper$no_job_id, x = content %>% toString)
}else{
valid_no_items <- FALSE
}
if(nodes_len == 0 & iterNr == 1 & !valid_no_items){
flog.error(
msg = glue::glue("Amount nodes for comp_name:'{name}' is amount_nodes:{nodes %>% length} for url:'{url}'."),
name = logger_name
)
}else if(valid_no_items){
msg <- glue::glue("Amount nodes for comp_name:'{name}' is amount_nodes:{nodes %>% length} for url:'{url}'. Found id valid_no_items:'{scraper$no_job_id}'.")
flog.info(
msg = msg,
name = logger_name
)
}else{
flog.info(
msg = glue::glue("Amount nodes for comp_name:'{name}' is amount_nodes:{nodes %>% length} for url:'{url}'."),
name = logger_name
)
}
}
#flog.info(msg = "Logger successfully initialized within function definition.", logger= logger_name)
# aaa <- function(){
#   b()
# }
#
# b <- function(){
#   tryCatchLog(expr = sum("a"*b), error = function(e){
#   })
# }
#
#
# aaa()
scrape_log_info <- function(target_name, url, msg, logger_name){
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following log is written: {msg}")
flog.info(msg = msg, name = logger_name)
}
scrape_log_warn <- function(target_name, url, msg, logger_name){
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following log is written: {msg}")
flog.warn(msg = msg, name = logger_name)
}
scrape_log_error <- function(target_name, url, msg, logger_name){
if(is.null(url)){
msg <- glue::glue("Missing url for comp_name:'{target_name}' Provided url is NULL - please provide a valid url.")
}else{
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following error was recorded: {msg}") %>%
toString
}
flog.error(msg = msg, name = logger_name)
}
library(futile.logger)
library(SteveAI)
SteveAI::run()
for(nr in seq(rvestScraper)){
print(nr)
scraper <- rvestScraper[[nr]]
name <- names(rvestScraper)[nr]
file_Name <- which(name == nms) %>% names
if(!length(file_Name)){
warning("no such file with response.")
next
}
load(file.path(folder_name, file_Name))
start <- Sys.time()
print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper),
error = function(e){
print(e)
name <- names(rvestScraper)[nr]
url <- rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
if(is.null(data_raw[[name]]) | length(data_raw[[name]]) < 2) next
# data_raw
# has_Id <- FALSE #names(data_raw[[name]])
# if(has_Id){
#
# }else{
#
#   # build id - take all non-empty columns that do not have blacklist items
#   apply(data_raw, 2, nchar)
#
# }
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
# fileNameJobDesc = paste0("dataRvest/JobDescLinks_", name, "_", Sys.Date(), ".csv")
# write.csv2(x = data_raw[[name]], file = fileNameJobDesc, row.names = FALSE)
# write.table(
#   data.frame(
#     name = name,
#     duration = scrapeDuration
#   ),
#   file = durationFileName,
#   append = TRUE,
#   row.names = FALSE,
#   col.names = FALSE
# )
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
if(!is.null(out)){
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = scraper$url,
logger_name = logger_name
)
}else{
warning(glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."))
scrape_log_warn(
target_name = name,
url = url,
msg = glue::glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."),
logger_name = logger_name
)
}
}
print(nr)
scraper <- rvestScraper[[nr]]
name <- names(rvestScraper)[nr]
file_Name <- which(name == nms) %>% names
if(!length(file_Name)){
warning("no such file with response.")
next
}
load(file.path(folder_name, file_Name))
start <- Sys.time()
print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper),
error = function(e){
print(e)
name <- names(rvestScraper)[nr]
url <- rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
if(is.null(data_raw[[name]]) | length(data_raw[[name]]) < 2) next
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
out
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = scraper$url,
logger_name = logger_name
)
amtItems <- apply(out, 2, nchar)
# todo: dirty
if(is.null(dim(amtItems))) amtItems <- matrix(amtItems, nrow = 1)
with_data <- amtItems %>%
apply(MARGIN = 2, FUN = sum) %>%
which01() %>%
out[, .]
job_id <- with_data[names(with_data) != "date"] %>%
apply(MARGIN = 1, FUN = paste, collapse = "__")
out <- cbind(job_id, with_data)
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
data_to_store <- out[!is.null(out)] # todo: can i safely remove this?  & lengths(out) > 1 --> counter example: what if i have only one row to add.
# todo: more special characters?
# chinese characters --> cant save as native encoding
data_to_store$job_id %<>%
trimws %>%
gsub(pattern = "||", replacement = "__", fixed = TRUE)
data_to_store$job_id %<>% gsub(pattern = ",|.|:", replacement = "", fixed = TRUE)
tbl_exists <- target_table_job %in% RSQLite::dbListTables(conn)
tbl_exists
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
new_jobs <- data_to_store[!(data_to_store$job_id %in% fetch_jobid$job_id), ]
# could also append = TRUE new_jobs. But new_jobs will not always have all required columns - not sure what the impacts
# of always overwriting the data is.
to_upload <- dplyr::bind_rows(
dplyr::mutate_all(fetch_jobid, as.character),
dplyr::mutate_all(new_jobs, as.character)
)
missing_Cols <- names(new_jobs)[!(names(new_jobs) %in% names(fetch_jobid))]
# Fehler: near ".1": syntax error --> error handling
dupl_col <- new_jobs %>%
names %>%
grepl(pattern = "[.]") %>%
which
dupl_col
if(length(dupl_col)){
warning(glue("duplicate columns with name: {names(new_jobs)[dupl_col]} is/are removed."))
new_jobs[[dupl_col]] <- NULL
missing_Cols <- names(new_jobs)[!(names(new_jobs) %in% names(fetch_jobid))]
}
statem <- glue("ALTER TABLE {target_table_job} ADD COLUMN {missing_Cols} TEXT")
sapply(statem, dbExecute, conn = conn)
# cant upload new_jobs only
n_jobs <- new_jobs %>%
unlist %>%
length
n_jobs
new_jobs
conn
SteveAI::run_multi()
load(file.path(scrape_Dir, "scraperRvestMultiPage.RData"))
scrape_Dir <- "~"
#source(file.path(scrape_Dir, "rvestFunctions.R"))
setwd(scrape_Dir)
load(file.path(scrape_Dir, "scraperRvestMultiPage.RData"))
SteveAI::run_multi()
library(SteveAI)
SteveAI::run_multi()
library(SteveAI)
data()
data(SteveAI)
data("SteveAI")
SteveAI:::rvestScraper
library(SteveAI)
scraper_rvest
steveai::scraper_rvest
SteveAI::rvestScraper
rvestScraper::rvestScraper
SteveAI::rvestScraper
load(file.path(scrape_Dir, "scraperRvestMultiPage.RData"))
scrape_Dir <- "~"
#source(file.path(scrape_Dir, "rvestFunctions.R"))
setwd(scrape_Dir)
load(file.path(scrape_Dir, "scraperRvestMultiPage.RData"))
SteveAI::scraperRvestMultiPage
load("C:/Users/User11/Documents/SteveAI/inst/extdata/scraperRvestMultiPage.RData")
load("C:/Users/User11/Documents/SteveAI/inst/extdata/scraperRvestMultiPage.RData")
SteveAI:::scrapeRvestMPage()
library(SteveAI)
load("C:/Users/User11/Documents/SteveAI/inst/extdata/scraper_rvest.RData")
load("C:/Users/User11/Documents/SteveAI/inst/extdata/scraperRvestMultiPage.RData")
?data
data(package = "SteveAI")
system.file('extdata', package = 'my_package')
system.file('extdata', package = 'SteveAI')
system.file('extdata', package = 'SteveAI') %>% list.files
library(SteveAI)
data(package ="SteveAI")
library(SteveAI)
scraperRvestMultiPage
library(SteveAI)
SteveAI::run()
SteveAI::run()
SteveAI::rvestScraper
start <- Sys.time()
# SteveAI_dir <- "/home/pi/sivis/scrape"
SteveAI_dir <- "~"
setwd(SteveAI_dir)
# load(file.path(SteveAI_dir, "scraper_rvest.RData"))
logger_name <- "sivis"
if(!dir.exists("dataRvest")){
dir.create("dataRvest")
}
flog.info(msg = "Logger successfully initialized from calling script.", logger= logger_name)
rvestScraping <- function(response, scraper){
# check direct is its xml?
if(is.character(response)){
stop("The provided response is not of type xml document, but of type character. The download might have failed, check the downloaded doument for validity.")
}
status <- response %>% httr::status_code()
url = scraper$url
log_status(
status = status,
name = name,
url = url
)
encoding <- response$headers$`content-type` %>%
ifelse(is.null(.), yes = "", no = .) %>%
strsplit(split = "charset=") %>%
unlist %>%
.[2]
if(is.na(encoding)){
scrape_log_info(
target_name = name,
url = url,
msg = glue::glue("No encoding found, defaulting to UTF-8"),
logger_name = logger_name
)
}
content <- response %>% httr::content(type = "text", encoding = encoding)
content_len <- content %>% nchar
nodes <- content %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = scraper$jobNameXpath)
log_node_len(nodes, name = name, scraper = scraper, content = content)
rvestOut <- gsub(
pattern = "\n|   |\tNew|\t",
replacement = "",
x = nodes %>% rvest::html_text()
)
# rvestLink <- getRvestHref(
#   url = url,
#   XPath = XPath
# )
if(length(rvestOut)){
out <- data.frame(
# x = rvestLink,
jobName = rvestOut,
comp = name,
date = Sys.Date(),
location = "",
eingestelltAm = "",
bereich = ""
)
}else{
out <- NULL
}
return(out)
}
scrape_log_info <- function(target_name, url, msg, logger_name){
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following log is written: {msg}")
flog.info(msg = msg, name = logger_name)
}
scrape_log_warn <- function(target_name, url, msg, logger_name){
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following log is written: {msg}")
flog.warn(msg = msg, name = logger_name)
}
scrape_log_error <- function(target_name, url, msg, logger_name){
if(is.null(url)){
msg <- glue::glue("Missing url for comp_name:'{target_name}' Provided url is NULL - please provide a valid url.")
}else{
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following error was recorded: {msg}") %>%
toString
}
flog.error(msg = msg, name = logger_name)
}
print(Sys.time())
data_raw <- list()
nr <- 1
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{Sys.Date()}.csv")
nr <- 60
folder_name <- glue("response_raw/{Sys.Date()}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 1
#
for(get_nr in 1:seq(SteveAI::rvestScraper)){
print(get_nr)
target_name <- names(SteveAI::rvestScraper)[get_nr]
scraper = SteveAI::rvestScraper[[get_nr]]
response <- tryCatchLog(
expr = scraper$url %>% httr::GET(),
error = function(e){
scrape_log_error(
target_name = target_name,
msg = e,
url = scraper$url,
logger_name = logger_name
)
}
)
file_Name <- glue("{names(SteveAI::rvestScraper)[get_nr]}_{Sys.Date()}.RData")
save(response, file = glue("{folder_name}/{file_Name}"))
}
responses <- list.files(folder_name)
seq(SteveAI::rvestScraper)
1:seq(SteveAI::rvestScraper)
seq(SteveAI::rvestScraper)
#
for(get_nr in seq(SteveAI::rvestScraper)){
print(get_nr)
target_name <- names(SteveAI::rvestScraper)[get_nr]
scraper = SteveAI::rvestScraper[[get_nr]]
response <- tryCatchLog(
expr = scraper$url %>% httr::GET(),
error = function(e){
scrape_log_error(
target_name = target_name,
msg = e,
url = scraper$url,
logger_name = logger_name
)
}
)
file_Name <- glue("{names(SteveAI::rvestScraper)[get_nr]}_{Sys.Date()}.RData")
save(response, file = glue("{folder_name}/{file_Name}"))
}
library(SteveAI)
SteveAI::run()
runif(100)
x <- runif(100, min = -1, max = 1)
y <- runif(100, min = -1, max = 1)
x^2 + y^2 < 1
x <- runif(100000000, min = -1, max = 1)
y <- runif(100000000, min = -1, max = 1)
x^2 + y^2 < 1
sum(d)
d <- x^2 + y^2 < 1
sum(d)
length(d)
sum(d) / length(d)
x <- runif(100000000, min = 0, max = 1)
y <- runif(100000000, min = 0, max = 1)
d <- x^2 + y^2 < 1
sum(d)
sum(d) / length(d)
sum(d) / length(d)*4
y <- runif(1000000000000, min = 0, max = 1)
x <- runif(1000000000000, min = 0, max = 1)
x <- runif(100000000000, min = 0, max = 1)
x <- runif(10000000000, min = 0, max = 1)
x <- runif(1000000000, min = 0, max = 1)
y <- runif(1000000000, min = 0, max = 1)
x <- runif(100000000, min = 0, max = 1)
y <- runif(100000000, min = 0, max = 1)
d <- x^2 + y^2 < 1
sum(d) / length(d)*4
x <- runif(100000000, min = 0, max = 1)
y <- runif(100000000, min = 0, max = 1)
d <- x^2 + y^2 < 1
sum(d) / length(d)*4
library(magrittr)
runif(1000, min = 0, max = 1)^2 + runif(1000, min = 0, max = 1)^2 < 1 %>%
{sum(.) / length(.)*4}
runif(1000, min = 0, max = 1)^2 + runif(1000, min = 0, max = 1)^2 < 1 %>%
{sum(.) / length(.)*4}
1:4 %>% {sum(.)}
1:4 %>% {sum(.) / length(.)}
runif(10, min = 0, max = 1)^2 + runif(10, min = 0, max = 1)^2 < 1
runif(10, min = 0, max = 1)^2 + runif(10, min = 0, max = 1)^2 < 1 %>%
{sum(.) / length(.)*4}
runif(10, min = 0, max = 1)^2 + runif(10, min = 0, max = 1)^2 < 1 %>%
{sum(.) / length(.)}
(runif(10, min = 0, max = 1)^2 + runif(10, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)}
(runif(10, min = 0, max = 1)^2 + runif(10, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
(runif(10000, min = 0, max = 1)^2 + runif(10000, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
(runif(10000, min = 0, max = 1)^2 + runif(10000, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
(runif(10000, min = 0, max = 1)^2 + runif(10000, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
(runif(10000, min = 0, max = 1)^2 + runif(10000, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
(runif(10000, min = 0, max = 1)^2 + runif(10000, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
(runif(10000, min = 0, max = 1)^2 + runif(10000, min = 0, max = 1)^2 < 1) %>%
{sum(.) / length(.)*4}
library(SteveAI)
SteveAI::run()
remove.packages("SteveAI")
remotes::install_github("Toniiiio/SteveAI")
remotes::install_github("Toniiiio/SteveAI")
