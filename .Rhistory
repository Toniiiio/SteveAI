indicators <- c("m/w/x", "m/f/d", "w/m/d", "m/w/d", "stelle anzeigen", "vollzeit", "vollzeit/teilzeit", "treffer pro seite", "(Junior) ", "(Senior) ")
add_indicators <- match_strings(target_strings = indicators, html_text = html_text)
indeed_jobs <- approx_match_strings(target_strings = job_titles, html_text = html_text)
apply_button <- has_apply_button(doc)
job_name_db <- matches_job_name_db(doc = doc, html_text = html_text, db_name = "rvest_scraper.db", target_table_job = "RVEST_SINGLE_JOBS")
list(
add_indicators = add_indicators,
indeed_jobs = indeed_jobs,
apply_button = apply_button,
job_name_db = job_name_db
)
}
filter_links <- function(links, domain, parsed_links, filter_domain = FALSE){
# some links are reported like: "//www.saturn.de/de/category/_teegeräte-476120.html".
needs_start <- substring(text = links$href, first = 1, last = 1) == "/" &
!grepl(x = links$href, pattern = "http") &
!grepl(x = links$href, pattern = "www.", fixed = TRUE)
links$href %<>%
{ifelse(test = needs_start, yes = paste0("https://www.", domain, .), no = .)}
domains <- links$href %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
# e.g. jobs.lidl.de should be same_domain as lidl.de - therefore use grepl instead of "=="
same_domain <- is.na(domains) | grepl(pattern = domain, x = domains)
hash_link <- substr(links$href, 1, 1) == "#"
is_empty <- !nchar(links$href)
is_na <- is.na(links$href)
alr_exist <- links$href %in% parsed_links
is_html <- sapply(c("mailto:", "javascript:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = tolower(links$href)) %>%
rowSums %>%
magrittr::not()
#!is_na & --> need NA for click with selenium
# links2 <- links
# links <- links2
keep <- !hash_link & !is_empty & !alr_exist & is_html
keep[is.na(keep)] <- TRUE
links %<>%
.[keep, ] %>%
{.[!duplicated(.), ]}
if(filter_domain) links %<>% .[same_domain, ]
# links %>% grepl(pattern = "jobs") %>% which %>% {links2[.]}
not_matched <- grepl(substring(text = links$href, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links$href, pattern = "https://") &
!grepl(links$href, pattern = "www.")
links$href %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
sort_links <- function(links){
if(!dim(links)[1]){
warning("No links to sort: links are empty.")
return(links)
}
# urls have to be case sensitive, see https://www.saturn.de/webapp/wcs/stores/servlet/MultiChannelAllJobsOverview.
links$text <- tolower(links$text)
direct_match <- c("(?=.*jobs)(?=.*suche)", "(?=.*jobs)(?=.*suche)(?=.*page=)", "(?=.*jobs)(?=.*suche)") %>% paste(collapse = "|")
# todo: könnte reihenfolge hier reinbringen - stellenangebote vor "über uns"
prioritize <- c("stellenmarkt", "current-vacancies", "vacancies", "bewerber", "jobfinder ", "stellen suchen", "jobbörse", "jobboerse", "jobs", "job", "all-jobs", "jobsuche","offenestellen", "offene-stellen", "stellenangebote", "job offers", "careers", "karriere", "beruf", "über uns", "ueber uns", "ueber-uns", "uber ", "über ", "ueber ")
de_prioritize <- c("impressum", "nutzungsbedingungen", "kontakt", "standort", "veranstaltungen", "newsletter", "datenschutz")
# lapply(direct_match, function(direct) lapply(links$href, grepl, perl = TRUE, pattern = direct))
direct <- sapply(links$href, grepl, perl = TRUE, pattern = direct_match, USE.NAMES = FALSE) %>%
which
href <- sapply(unique(prioritize), stringr::str_count, string = tolower(links$href))
href[is.na(href)] <- 0
text <- sapply(unique(prioritize), stringr::str_count, string = links$text)
text[is.na(text)] <- 0
# todo: only heuristic
dims <- dim(text)
weights <- matrix(rep(rev(seq(dims[2])^2), dims[1]), nrow = dims[1], byrow = TRUE)
all <- as.matrix(href + text)
first <- as.matrix(all*weights) %>%
rowSums(na.rm = TRUE) %>%
{order(., decreasing = TRUE)[0:sum(. > 0)]}
# unlist %>%
# table %>% sort(decreasing = TRUE) %>%
# {links[as.numeric(names(.))]}
links[first, ]
href <- sapply(unique(de_prioritize), stringr::str_count, string = tolower(links$href))
text <- sapply(unique(de_prioritize), stringr::str_count, string = links$text)
# todo: only heuristic so far
dims <- dim(text)
weights <- matrix(rep(rev(seq(dims[2])^2), dims[1]), nrow = dims[1], byrow = TRUE)
all <- as.matrix(href + text)
last <- as.matrix(all*weights) %>%
rowSums(na.rm = TRUE) %>%
{order(., decreasing = TRUE)[0:sum(. > 0)]}
iframe <- which(links$text == "iframe")
order <- c(iframe, direct, first, setdiff(seq(links$href), c(first, last, direct)),last) %>%
unique
return(links[order, ])
}
url <- "https://www.tesa.de"
remDr <- start_selenium()
xx <- find_job_page(url, remDr, use_selenium = TRUE)
xx$winner
xx$counts
recover()
all_docs[[id]] <- get_doc_selenium(link, remDr)
all_docs[[id]]
a <- function() return(c(1,2))
a()
a,b = a()
url <- "https://www.daimler.de/"
#url <- "https://www.daimler.de/"
find_job_page <- function(url, remDr, use_selenium = TRUE){
domain <- urltools::domain(url) %>%
gsub(pattern = "www.", replacement = "")
parsed_links <- data.frame(href = character(), text = character())
links_per_level <- list()
docs_per_level <- list()
if(use_selenium){
out <- get_doc_selenium(url, remDr)
doc <- out$doc
domain <- out$domain
}else{
doc <- get_doc(url)
}
tags <- doc %>% html_nodes(xpath = "//*[self::a or self::button or self::input]")
txt <- tags %>% html_text()
val <- tags %>% html_attr(name = "value") %>% ifelse(is.na(.), "", .)
links <- data.frame(text = paste0(txt, val), href = tags %>% html_attr(name = "href"))
if(!dim(links)[1]) stop("No links found")
links %>% grepl(pattern = "jobs") %>% sum
head(links)
iter_nr <- 1
#links_per_level[1] <- url
html_texts <- list()
links <- filter_links(links = links, domain = domain, parsed_links = parsed_links)
links
# catch document and all links
all_links <- list()
all_docs <- list()
matches <- list()
links <- sort_links(links)
links %>% head
#links[1] %>% browseURL()
iter_nr <- 0
max_iter <- 10
counts <- rep(NA, max_iter)
while(iter_nr < max_iter){
iter_nr <- iter_nr + 1
target_link <- links[1, ]
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- htm2txt::htm2txt(as.character(all_docs[[id]]))
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
warning("No content")
}
#all_docs[[id]] %>% SteveAI::showHtmlPage()
# html_texts[[id]] %>% cat
# doc <- "https://www.bofrost.de/karriere/job/" %>% read_html
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# "https://careers.danone.com/de-global/" %in% links$href
# "https://careers.danone.com/de-global/" %in% links2$href
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links <- rbind(parsed_links, target_link)
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links <- rbind(parsed_links, links)
exclude <- duplicated(links$href) | duplicated(links$href, fromLast = TRUE)
links <- links[!exclude, ] %>% sort_links()
links$href %>% grepl(pattern = "stellenangebote")
# taleo careersection jobsearch
head(links)
links$href <- gsub(x = links$href, pattern = "www.www.", replacement = "www.", fixed = TRUE)
head(links)
}
winner <- which(max(counts, na.rm = TRUE) == counts)[1]
# targets <- parsed_links[max(counts, na.rm = TRUE) == counts]
# targets[1] %>% browseURL()
return(
x <- list(
doc = as.character(all_docs[[winner]]),
all_docs = lapply(all_docs, as.character),
counts = counts,
parsed_links = parsed_links,
matches = matches,
winner = winner
)
)
}
url <- "https://www.tesa.de"
xx <- find_job_page(url, remDr, use_selenium = TRUE)
xx$winner
xx$counts
xx$parsed_links[xx$winner]
xx$parsed_links
xx$parsed_links$href[xx$winner]
xx$parsed_links$href[xx$winner] %>% browseURL()
library(rvest)
library(httr)
library(magrittr)
library(urltools)
source("R/is_job_offer_page.R")
source("R/handle_links.R")
source("R/link_checker.R")
load("data/comp_urls_indeed.RData")
load("data/job_page_candidates_indeed.RData")
urls <- unlist(comp_urls)
if(!exists("indeed_reuslts")){
indeed_results <- list()
}
source("R/handle_links.R")
source("R/link_checker.R")
load("data/comp_urls_indeed.RData")
load("data/job_page_candidates_indeed.RData")
urls <- unlist(comp_urls)
if(!exists("indeed_reuslts")){
indeed_results <- list()
}
remDr <- start_selenium(port = 4457)
remDr <- start_selenium(port = 4457)
names(indeed_results) %>% .[length(.)] %>%
magrittr::equals(urls) %>% which
nr <- 1
for(nr in seq(urls)[1:3672]){
url <- urls[nr]
indeed_results[[url]] <- tryCatch(
find_job_page(url, remDr, TRUE),
error = function(e){
remDr <- tryCatch(start_selenium (port = 4455 + nr), error = function(e) return(""))
return("")
}
)
save(indeed_results, file = "data/job_page_candidates_indeed.RData")
}
Q
indeed_results[[url]]
indeed_results[[url]]
indeed_results
indeed_results[[1]]
indeed_results[[2]]
indeed_results[[2]]$winner
indeed_results[[2]]$parsed_links
indeed_results[[2]]$counts
indeed_results[[2]]$matches
indeed_results[[3]]
indeed_results[[4]]
indeed_results[[5]]
indeed_results[[6]]
url
url <- "https://aap.de"
parsed_links <- data.frame(href = character(), text = character())
links_per_level <- list()
docs_per_level <- list()
if(use_selenium){
out <- get_doc_selenium(url, remDr)
doc <- out$doc
domain <- out$domain
}else{
doc <- get_doc(url)
}
url
remDr <- start_selenium(port_nr = 4459)
out <- get_doc_selenium(url, remDr)
doc <- out$doc
domain <- out$domain
tags <- doc %>% html_nodes(xpath = "//*[self::a or self::button or self::input]")
txt <- tags %>% html_text()
val <- tags %>% html_attr(name = "value") %>% ifelse(is.na(.), "", .)
links <- data.frame(text = paste0(txt, val), href = tags %>% html_attr(name = "href"))
if(!dim(links)[1]) stop("No links found")
links %>% grepl(pattern = "jobs") %>% sum
head(links)
iter_nr <- 1
html_texts <- list()
links <- filter_links(links = links, domain = domain, parsed_links = parsed_links)
links
# catch document and all links
all_links <- list()
all_docs <- list()
matches <- list()
links <- sort_links(links)
links %>% head
#links[1] %>% browseURL()
iter_nr <- 0
max_iter <- 10
counts <- rep(NA, max_iter)
iter_nr <- iter_nr + 1
target_link <- links[1, ]
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- htm2txt::htm2txt(as.character(all_docs[[id]]))
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
warning("No content")
}
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# "https://careers.danone.com/de-global/" %in% links$href
# "https://careers.danone.com/de-global/" %in% links2$href
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links <- rbind(parsed_links, target_link)
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links <- rbind(parsed_links, links)
target_link
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links <- rbind(parsed_links, links)
links
exclude <- duplicated(links$href) | duplicated(links$href, fromLast = TRUE)
exclude
links <- links[!exclude, ] %>% sort_links()
links
target_link
target_link
target_link
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links
target_link
target_link$href
target_link$href %>% nchar
target_link$href
target_link$href %>% {substring(., nchar(.))}
last_char <- link %>% {substring(., nchar(.))}
last_char <- link %>% {substring(., nchar(.))}
last_char
last_char <- substring(link, first = nchar(link))} == "/"
rm_last_char <- substring(link, first = nchar(link))} == "/"
rm_last_char <- substring(link, first = nchar(link)) == "/"
rm_last_char
ifelse(last_char, yes = substring(link, first = 1, last = nchar(link) - 1))
link <- target_link$href
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(last_char, yes = substring(link, first = 1, last = nchar(link) - 1))
ifelse(last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
filter_links <- function(links, domain, parsed_links, filter_domain = FALSE){
# some links are reported like: "//www.saturn.de/de/category/_teegeräte-476120.html".
needs_start <- substring(text = links$href, first = 1, last = 1) == "/" &
!grepl(x = links$href, pattern = "http") &
!grepl(x = links$href, pattern = "www.", fixed = TRUE)
links$href %<>%
{ifelse(test = needs_start, yes = paste0("https://www.", domain, .), no = .)}
domains <- links$href %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
# e.g. jobs.lidl.de should be same_domain as lidl.de - therefore use grepl instead of "=="
same_domain <- is.na(domains) | grepl(pattern = domain, x = domains)
hash_link <- substr(links$href, 1, 1) == "#"
is_empty <- !nchar(links$href)
is_na <- is.na(links$href)
alr_exist <- links$href %in% parsed_links
is_html <- sapply(c("mailto:", "javascript:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = tolower(links$href)) %>%
rowSums %>%
magrittr::not()
#!is_na & --> need NA for click with selenium
# links2 <- links
# links <- links2
keep <- !hash_link & !is_empty & !alr_exist & is_html
keep[is.na(keep)] <- TRUE
links %<>%
.[keep, ] %>%
{.[!duplicated(.), ]}
if(filter_domain) links %<>% .[same_domain, ]
# links %>% grepl(pattern = "jobs") %>% which %>% {links2[.]}
not_matched <- grepl(substring(text = links$href, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links$href, pattern = "https://") &
!grepl(links$href, pattern = "www.")
links$href %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
link <- target_link$href
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
links
links$href <- sapply(links$href, equalize_links)
equalize_links <- function(link){
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
}
links$href <- sapply(links$href, equalize_links)
links$href
equalize_links <- function(link){
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
}
filter_links <- function(links, domain, parsed_links, filter_domain = FALSE){
links$href <- sapply(links$href, equalize_links)
# some links are reported like: "//www.saturn.de/de/category/_teegeräte-476120.html".
needs_start <- substring(text = links$href, first = 1, last = 1) == "/" &
!grepl(x = links$href, pattern = "http") &
!grepl(x = links$href, pattern = "www.", fixed = TRUE)
links$href %<>%
{ifelse(test = needs_start, yes = paste0("https://www.", domain, .), no = .)}
domains <- links$href %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
# e.g. jobs.lidl.de should be same_domain as lidl.de - therefore use grepl instead of "=="
same_domain <- is.na(domains) | grepl(pattern = domain, x = domains)
hash_link <- substr(links$href, 1, 1) == "#"
is_empty <- !nchar(links$href)
is_na <- is.na(links$href)
alr_exist <- links$href %in% parsed_links
is_html <- sapply(c("mailto:", "javascript:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = tolower(links$href)) %>%
rowSums %>%
magrittr::not()
#!is_na & --> need NA for click with selenium
# links2 <- links
# links <- links2
keep <- !hash_link & !is_empty & !alr_exist & is_html
keep[is.na(keep)] <- TRUE
links %<>%
.[keep, ] %>%
{.[!duplicated(.), ]}
if(filter_domain) links %<>% .[same_domain, ]
# links %>% grepl(pattern = "jobs") %>% which %>% {links2[.]}
not_matched <- grepl(substring(text = links$href, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links$href, pattern = "https://") &
!grepl(links$href, pattern = "www.")
links$href %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
nr
nr <- 2
url <- urls[nr]
url
nr <- 3
url <- urls[nr]
url
url <- urls[nr]
indeed_results[[url]] <- tryCatch(
find_job_page(url, remDr, TRUE),
error = function(e){
remDr <- tryCatch(start_selenium (port = 4455 + nr), error = function(e) return(""))
return("")
}
)
remDr <- start_selenium(port = 4457)
names(indeed_results) %>% .[length(.)] %>%
magrittr::equals(urls) %>% which
nr <- 3
url <- urls[nr]
indeed_results[[url]] <- tryCatch(
find_job_page(url, remDr, TRUE),
error = function(e){
remDr <- tryCatch(start_selenium (port = 4455 + nr), error = function(e) return(""))
return("")
}
)
Q
