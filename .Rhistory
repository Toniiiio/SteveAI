rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
dont_run <- function(){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI/"
setwd(SteveAI_dir)
load(file.path(SteveAI_dir, "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue("{SteveAI_dir}rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
bb <- log_data[which(log_data$comp_name %>% is.na), ]
log_data$n_nodes
log_data %<>%
dplyr::group_by(comp_name) %>%
dplyr::mutate(db_consist = (sum(n_today_jobs, na.rm = TRUE) + sum(n_duplicate_jobs, na.rm = TRUE) == sum(n_nodes, na.rm = TRUE)))
log_data$db_consist
error_terms <- c("wrong_xpath", "diff_length_links_id", "wrong_url", "status_404",
"timeout", "no_encod", "func_miss", "curl_error", "connect_reset",
"no_resolve_host")
ui = fluidPage({
mainPanel(
tabsetPanel(
tabPanel("Overview", fluidRow(
numericInput(
inputId = "min_nodes",
label = "Min amt nodes:",
min = 0,
value = 0,
max = 500
),
selectInput(
inputId = "logicals",
label = "Filter logicals",
choices = names(error_type_ident),
multiple = TRUE,
selectize = TRUE,
selected = NULL
),
checkboxInput(
inputId = "obj_has_txt",
label = "Missing object in code",
value = FALSE
),
DTOutput('tbl_all')
)
),
tabPanel(
title = "Analyse",
uiOutput("items"),
DTOutput('tbl_single'),
fluidRow(
column(width = 3,
checkboxGroupInput(inputId = "error_terms", label = "Error terms", choices = error_terms, selected = error_terms),
actionButton(inputId = "open_scrape_url", label = "Open current url:"),
actionButton(inputId = "google_search", label = "Search for new page at google:"),
actionButton(inputId = "remove", label = "Remove this item"),
actionButton(inputId = "curl", label = "Try curl in cmd."),
actionButton(inputId = "domain", label = "Ping domain."),
shiny::fluidRow(
textInput(inputId = "new_url", label = "New url:"),
actionButton(inputId = "add_new_url", label = "Add new url!")
),
textInput(inputId = "target_text", label = "Text for xpath:", value = "m/w"),
uiOutput("xpath"),
actionButton(inputId = "get_xpath", label = "Get xpath"),
actionButton(inputId = "use_xpath", label = "Use xpath"),
verbatimTextOutput(outputId = "curl"),
textInput(inputId = "no_job_id", label = "Text to identify valid no jobs:", value = ""),
actionButton(inputId = "add_nojob_id", label = "Add no job id"),
actionButton(inputId = "finish_item", label = "Mark as done."),
),
column(width = 9,
htmlOutput("frame")
)
)
)
)
)
})
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
keep <- which((error_items[, error_terms] %>% rowSums()) > 0)
error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_comps = error_comps, xpath_output = NULL)
observe({
keep <- which((error_items[, input$error_terms] %>% rowSums()) > 0)
global$error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
})
output$frame <- renderUI({
tags$iframe(src = rvestScraper[[input$error_item]]$url, height = 600, width = 1500)
})
output$items <- renderUI({
print(length(global$error_comps))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_comps,
selected = global$error_comps[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_comps)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_comps <- setdiff(global$error_comps, input$error_item)
print(length(unlist(global$error_comps)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
print("input$error_item")
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(c(input$target_text, input$get_xpath, input$error_item, global$doc), {
req(nchar(input$target_text) > 0)
req(!is.null(global$doc))
print("input$target_text")
print(input$target_text)
print("doc")
print(global$doc)
print("zzz")
is_json <- global$doc %>% jsonlite::validate()
if(is_json){
warning("document is of type json")
shinyalert(text = "document is of type json - changing to '<html>'")
global$doc <- "<html>"
txt <- "/html"
}else{
txt <- SteveAI::getXPathByText(doc = global$doc, text = input$target_text)
}
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
print("rr")
})
observeEvent(input$xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- global$items
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
output$curl <- renderPrint(global$curl_output)
})
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
output$curl <- renderPrint(global$domain_output)
})
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
}
dont_run()
url <- 'https://jobs.definiens.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=&optionsFacetsDD_department=&optionsFacetsDD_customfield1='
url %>% httr::GET()
library(rvest)
library(httr)
library(magrittr)
library(glue)
library(urltools)
library(SteveAI)
source("SteveAI/R/is_job_offer_page.R")
source("R/is_job_offer_page.R")
source("SteveAI/R/handle_links.R")
source("R/handle_links.R")
source("SteveAI/R/link_checker.R")
source("R/build_scraper/link_checker.R")
source('R/build_scraper/target_text.R')
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
url <- rvestScraper$DEFINIENS$domain
xx <- find_job_page(url, ses = ses)
url <- 'https://jobs.definiens.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=&optionsFacetsDD_department=&optionsFacetsDD_customfield1='
url
url <- rvestScraper$DEFINIENS$domain
rvestScraper$DEFINIENS$domain
url <- rvestScraper$DEFINIENS$domain
url
rvestScraper$DEFINIENS <- NULL
save(rvestScraper, file = "scraper_rvest.RData")
dont_run()
url <- rvestScraper$BRENNTAG$domain
url
xx <- find_job_page(url, ses = ses)
xx <- find_job_page(url, ses = ses)
alarm()
alarm()
alarm()
alarm()
beep <- function(n = 3){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
beep()
beep <- function(n = 3){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
xx <- find_job_page(url, ses = ses)
beep()
url <- "http://www.hermes.de"  #rvestScraper[[comp_name]]$domain
url
xx <- find_job_page(url, ses = ses)
library(rvest)
library(httr)
library(magrittr)
library(glue)
library(urltools)
library(SteveAI)
source("SteveAI/R/is_job_offer_page.R")
source("R/is_job_offer_page.R")
source("SteveAI/R/handle_links.R")
source("R/handle_links.R")
source("SteveAI/R/link_checker.R")
source("R/build_scraper/link_checker.R")
source('R/build_scraper/target_text.R')
beep <- function(n = 2){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
# scrape searcher does not move forward:
#rvestScraper$APLANT$domain <- "https://www.sunbeltrentals.co.uk/"
comp_name <- "DEUTSCHE-BANK"
url <- "http://www.hermes.de"  #rvestScraper[[comp_name]]$domain
url
url %>% browseURL()
xx <- find_job_page(url, ses = ses)
beep()
url <- "http://www.hermes.de"  #rvestScraper[[comp_name]]$domain
url
xx <- find_job_page(url, ses = ses)
beep()
winner_url <- xx$parsed_links$href[xx$winner]
winner_url
paste0(winner_url, "#:~:text=", url_encode(xx$candidate_meta[[4]][1])) %>% browseURL()
xx
xx$parsed_links$href
url <- "http://www.lidl.de"  #rvestScraper[[comp_name]]$domain
url
url %>% browseURL()
xx <- find_job_page(url, ses = ses)
beep()
nonblocking <- isTRUE(getOption("curl_interrupt", TRUE))
output <- .Call(R_curl_fetch_memory, enc2utf8(url), handle,
nonblocking)
R_curl_fetch_memory
url
handle
nonblocking
output <- .Call(R_curl_fetch_memory, enc2utf8(url), handle,
nonblocking)
get_doc_phantom <- function(url, ses, pjs){
url <- as.character(url)
tryCatch(ses$go(url), error = function(e){
warning("Need to restart phantom webdriver")
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
ses$go(url)
})
url_before <- tryCatch(ses$getUrl(), error = function(e) return(""))
domain <- url_before %>%
urltools::domain()
# %>%
# gsub(pattern = "www.", replacement = "")
doc <- tryCatch(ses$findElement(xpath = "/*")$getAttribute(name = "innerHTML") %>%
xml2::read_html(), error = function(e){
return("")
})
return(
list(doc = doc, domain = domain, ses = ses, pjs = pjs, url_before = url_before)
)
}
parse_link <- function(target_link, iter_nr, link_meta, use_selenium = FALSE, use_phantom = TRUE, ses = NULL, remDr = NULL){
links <- link_meta$links
all_docs <- link_meta$all_docs
html_texts <- link_meta$html_texts
matches <- link_meta$matches
all_links <- link_meta$all_links
counts <- link_meta$counts
parsed_links <- link_meta$parsed_links
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
# workaround until frames are supported
if(is.na(link)){
exclude <- links$href %in% link
links <- links[!exclude, ]
message("skipping NA (frame link)")
return()
}
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(url = link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else if(use_phantom){
out <- get_doc_phantom(url = link, ses = ses, pjs = pjs)
ses <- out$ses
domain <- out$domain
url_before <- out$url_before
all_docs[[id]] <- out$doc
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- tryCatch(htm2txt::htm2txt(as.character(all_docs[[id]])),
error = function(e){
message(e)
warning(e)
return("")
})
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
message("No content")
warning("No content")
return(
list(
links = links[-1, ], all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
#all_docs[[id]] %>% SteveAI::showHtmlPage()
# html_texts[[id]] %>% cat
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# todo: cant find src in code - maybe have to use swith to frame but cant use it as new link then
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
# speculative change: dont replace all values
if(length(iframe_links_raw)){
iframe_links_raw[is.na(iframe_links_raw)] <- ""
}
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links[iter_nr, ]$href <- target_link$href
parsed_links[iter_nr, ]$text <- target_link$text
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links, filter_domain = FALSE)
links <- links[!duplicated(links$href), ]
links$href
links <- rbind(parsed_links, links)
head(links)
exclude <- links$href %in% parsed_links$href
head(exclude)
links <- links[!exclude, ] %>% sort_links()
# taleo careersection jobsearch
head(links$href)
# links$href <- gsub(x = links$href, pattern = "www.www.", replacement = "www.", fixed = TRUE)
# can fail
links <- check_for_button(links, url_before)
return(
list(
links = links, all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
check_for_button <- function(links, url_before, ses, pjs){
# if fail here restart ses and go to url_before
doc <- ses$findElement(xpath = "/*")
doc_len_before <- doc$getAttribute("innerHTML") %>% nchar
xp <- generate_button_xpath()
buttons <- ses$findElements(xpath = xp)
for(nr in seq(buttons)){
tryCatch(buttons[[nr]]$click(), error = function(e) message(e))
}
input_xpath <- "//input[@type = 'submit' or @value = 'Search Jobs' or contains(@value, 'Search') or @title = 'Search Jobs' or @value='Suche starten']"
inputs <- ses$findElements(xpath = input_xpath)
job_related_page_xp <- "//*[contains(text(), 'Find a job') or contains(text(), 'Search and apply') or contains(text(), 'Global Career Opportunities') or contains(text(), 'Search Jobs') or contains(text(), 'Search for jobs') or contains(text(), 'Find Jobs')  or contains(text(), 'Aktuelle Stellenangebote') or contains(text(), 'gewÃ¼nschte Stelle')  or contains(text(), 'job search') or contains(text(), 'Search Current Openings')  or contains(text(), 'Careers')]"
is_job_related <- ses$findElements(xpath = job_related_page_xp) %>% length
is_job_related
# todo: finish
#
button_xpath <- "//button[./parent::*//*[contains(text(), 'GO')] or @title = 'Search for Jobs']"
buttons <- ses$findElements(xpath = button_xpath)
if(is_job_related){
for(nr in seq(buttons)){
tryCatch(buttons[[nr]]$click(), error = function(e) message(e))
}
}
if(is_job_related){
for(nr in seq(inputs)){
message("Found and trying a relevant input")
tryCatch(inputs[[nr]]$click(), error = function(e) message(e))
}
}
alinks <- ses$findElements(xpath = "//a[@id = 'taleoSearchSubmit']")
if(is_job_related){
for(nr in seq(alinks)){
message("Found and trying a relevant input")
tryCatch(alinks[[nr]]$click(), error = function(e) message(e))
}
}
url_after <- ses$getUrl()
doc <- ses$findElement(xpath = "/*")
doc_len_after <- doc$getAttribute("innerHTML") %>% nchar
button_relevant <- url_after != url_before | doc_len_before != doc_len_after
if(button_relevant){
message("Found a relevant button")
links <- rbind(
data.frame(
href = url_after,
text = "Caused by SteveAI button click"
),
links
)
}
return(links)
}
