#   upload <- data_to_store[not(data_to_store$job_id %in% fetch$job_id), ]
#
# }
#
# DBI::dbWriteTable(
#   conn = conn,
#   name = target_table,
#   value = upload,
#   append = TRUE
# )
# fileNameJobDesc = paste0("dataMultiRvest/JobDescLinks_", name, "_", Sys.Date(), ".csv")
# write.csv2(x = do.call(what = rbind, args = output), file = fileNameJobDesc, row.names = FALSE)
# print("DONE - SAVING NOW")
}
run_multi <- function(){
scrape_Dir <- "~"
#source(file.path(scrape_Dir, "rvestFunctions.R"))
setwd(scrape_Dir)
load(file.path(scrape_Dir, "scraperRvestMultiPage.RData"))
folder_name <- glue("response_raw/{Sys.Date()}")
dir.create("response_raw")
dir.create("dataMultiRvest")
dir.create(folder_name)
nr <- 1
name <- "VNG"
name <- names(scraperRvestMultiPage)[50]
print(nr)
scraper <- scraperRvestMultiPage[[name]]
nr <- 1
out <- list()
scraperNr <- 15
durationFileName <- file.path(scrape_Dir, paste0("scrapeDuration_", Sys.Date(), ".csv"))
names(scraperRvestMultiPage)
for(scraperNr in 1:length(scraperRvestMultiPage)){ #1:
# if(scraperNr %in% c(1, 2, 3, 4, 35)) next
start <- Sys.time()
print(paste0("scraperNr: ", scraperNr))
name <- names(scraperRvestMultiPage)[scraperNr]
print(name)
out[[name]] <- tryCatchLog(
expr = scrapeRvestMPage(name = name),
error = function(e){
print(e)
# flog.error(msg = e, logger = logger_name)
}
)
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
# write.table(
#   data.frame(
#     name = name,
#     duration = scrapeDuration
#   ),
#   file = durationFileName,
#   append = TRUE,
#   col.names = FALSE,
#   row.names = FALSE
# )
}
}
isHtmlDoc <- function(elem){
all(names(elem) %in% c("node", "doc"))
}
getHTML <- function(getURL, targetString){
xx <- httr::GET(getURL)
response <- content(xx)
m <- str_count(string = response, pattern = "<.*>")
if(length(m) > 1){
docs <- lapply(response[m > 0], read_html)
}else{
docs <- response
}
if(isHtmlDoc((response))){
docs <- response
tags <- list(html_nodes(docs, xpath = paste0("//*[contains(text(), '", targetString,"')]")))
}else{
m2 <- lapply(docs, html_nodes, xpath = paste0("//*[contains(text(), '", targetString,"')]"))
tags <- m2[lengths(m2) > 0]
docs <- docs[lengths(m2) > 0]
}
list(
xpath = sapply(tags, getXPathByTag, text = targetString, exact = FALSE),
docs = docs
)
}
scrapeGetHtml <- function(getUrl, res){
htmlDoc <- getURL %>% GET %>% content
subsetBy <- names(res$xpath)
if(!is.null(subsetBy)) htmlDoc <- htmlDoc %$% get(subsetBy) %>% read_html
texts <- htmlDoc %>% html_nodes(xpath = unname(res$xpath)) %>% html_text
list(
texts = texts
)
}
extractJobLink <- function(hrefAttrText, baseUrl){
firstHrefPart <- strsplit(hrefAttrText, ";")[[1]][1]
hasJavascript <- grep(pattern = "javascript:", x = firstHrefPart)
if(length(hasJavascript)){
extractedRaw <- str_extract(
string = firstHrefPart,
pattern = "\\(([^)]+)\\)"
)
firstHrefPart <- gsub(pattern = "\\(|\\)|'", replacement = "", x = extractedRaw)
}
match <- substr(firstHrefPart, 1, 4) == "http"
if(sum(match) / length(match) < 1){
dom <- domain(baseUrl) # scraper[[compName]]$url
url <- paste0("https://", dom,  firstHrefPart)
}else{
url <- firstHrefPart
}
return(url)
}
getXPath <- function(url, text, exact = FALSE){
tagName <- ""
tags <- c()
if(exact){
xpath <- paste0("//*[text() = '", text,"']")
}else{
xpath <- paste0("//*[contains(text(), '", text,"')]")
}
tag <- read_html(url) %>% html_nodes(xpath = xpath)
if(!length(tag)){
print("Element not in source!")
return()
}
while(tagName != "html"){
tag <- tag[1] %>% html_nodes(xpath = "..")
tagName <- tag %>% html_name()
tags <- c(tags, tagName)
}
xpath <- paste(c("", tags[length(tags):1]), collapse ="/")
xpath
}
getXPathByTag <- function(tag, text, exact = FALSE){
tagName <- ""
tags <- c()
if(!length(tag)){
xpath = ""
}else{
if(length(tag) > 1){
tagNames <- sapply(tag, html_name)
match <- which(tagNames != "script")
if(!length(match)) match <- 1
tag <- tag[match[1]]
}
while(tagName != "html"){
tagName <- tag %>% html_name()
tags <- c(tags, tagName)
tag <- tag %>% html_nodes(xpath = "..")
}
xpath <- paste(c("", tags[length(tags):1]), collapse ="/")
}
xpath
}
getScrapeInfo <- function(browserOutputRaw){
splitted <- strsplit(
x = browserOutputRaw,
split = ";"
)[[1]]
browserOutput <- list(
url = splitted[1],
clickType = splitted[2],
expectedOutput = strsplit(splitted[3], split = "~~~")[[1]],
XPath = splitted[4],
XPathBlank = gsub(
pattern = "[[]\\d+[]]",
replacement = "",
x = splitted[4]
)
)
browserOutput
}
getRvestText <- function(url, XPath){
read_html(x = url) %>%
html_nodes(xpath = XPath) %>%
html_text()
}
getRvestHtmlSource <- function(url){
textRaw <- read_html(x = url) %>% html_nodes(xpath = "//*[not(*) and not(self::script)]") %>% html_text()
text <- paste(textRaw, collapse = "\n")
text
}
getRvestHref <- function(url, XPath){
read_html(x = url) %>%
html_nodes(xpath = XPath) %>%
html_attr(name = "href")
}
rvestScraping <- function(nr){
url <- scraper[[nr]]$url
XPath <- scraper[[nr]]$jobNameXpath
rvestOutRaw <- getRvestText(
url = url,
XPath = XPath
)
rvestOut <- gsub(
pattern = "\n",
replacement = "",
x = rvestOutRaw
)
rvestOut
}
urlGenWorkDays <- function(baseUrl, nr){
middlePart <- nr - 2
if(middlePart == -1) middlePart <- "fs"
paste0(
baseUrl, "/", middlePart,
"/searchPagination/318c8bb6f553100021d223d9780d30be/", (nr - 1)*ItemsPerPage
)
}
scrapeWorkDays <- function(url){
GETResult <- GET(url = url)
subset <- content(GETResult)$body$children[[1]]$children[[1]]$listItems
maxIter = length(subset)
jobTitle <- c()
location <- c()
id <- c()
eingestelltAm <- c()
for(nr in 1:maxIter){
jobTitle[nr] <- subset[[nr]]$title$instances[[1]]$text
location[nr] <- subset[[nr]]$subtitles[[1]]$instances[[1]]$text
id[nr] <- subset[[nr]]$subtitles[[2]]$instances[[1]]$text
eingestelltAm[nr] <- subset[[nr]]$subtitles[[3]]$instances[[1]]$text
}
data.frame(
jobTitle = jobTitle,
location = location,
id = id,
eingestelltAm = eingestelltAm
)
}
scheduledGET <- function(url, targetKeys, base, baseFollow = NULL){
if(is.null(base)){
stop("Parameter base, provided to scheduledGET(), is NULL - please provide a valid subset value.")
}
getRes <- GET(url = url)
if(getRes$status_code != 200) return(NULL)
contentGET <- content(getRes)
contentGETFlat <- unlist(contentGET)
if(is.null(contentGETFlat)) return(NULL)
splitNames <- names(contentGETFlat) %>% strsplit(split = "[.]")
lastKeys <- sapply(X = splitNames, FUN = tail, n = 1)
# todo;do i neeed two of these functions?
baseElems <- subsetByStr2(contentGET, base)
if(!length(baseElems)) return(NULL)
targetKey <- targetKeys[[1]]
texts <- lapply(targetKeys, function(targetKey){
baseElem <- baseElems[[1]]
raw <- sapply(baseElems, function(baseElem){
if(!is.null(baseFollow)){
baseElem <- subsetByStr3(lstRaw = baseElem, arr = baseFollow)
}
subsetByStr3(lstRaw = baseElem, arr = targetKey)
}, USE.NAMES = FALSE)
raw2 <- sapply(raw, paste, collapse = " | ") %>% unname
if(!length(raw2)) return(raw2)
df <- data.frame(raw2, stringsAsFactors = FALSE)
df <- setNames(df, paste(targetKey, collapse = "|"))
df
})
res <- do.call(what = cbind, texts)
colnames(res) <- targetKeys
rownames(res) <- NULL
list(
res = res,
base = base,
targetKeys = targetKeys
)
}
subsetByStr2 <- function(lstRaw, arr){
lst <- lstRaw
nr <- 1
for(nr in 1:length(arr)){
lst <-  lst[[arr[nr]]]
}
lst
}
subsetByStr3 <- function(lstRaw, arr){
lst <- lstRaw
nr <- 1
for(nr in 1:length(arr)){
lst <-  lst[[arr[nr]]]
if(!length(lst)) return("") # missing element return empty string
if(is.null(names(lst))) lst <- lst[[1]]
}
lst
}
showHtmlPage <- function(doc){
tmp <- tempfile(fileext = ".html")
doc %>% toString %>% writeLines(con = tmp)
tmp %>% browseURL(browser = rstudioapi::viewer)
}
getwd()
db_name <- "rvest_scraper.db"
file.exists(db_name)
setwd("~")
db_name <- "rvest_scraper.db"
db_exists <- file.exists(db_name)
db_exists
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
# 5.
found_tables <- dbListTables(conn)
# 5.
found_tables <- DBI::dbListTables(conn)
found_tables
expect_tables <- c("RVEST_MULTI_JOBS",  "RVEST_MULTI_TIME",  "RVEST_SINGLE_JOBS", "RVEST_SINGLE_TIME")
unknown_tables <- setdiff(found_tables, expect_tables)
unknown_tables
missing_tables <- setdiff(expect_tables, found_tables)
missing_tables
sapply(glue("SELECT COUNT(*) FROM {expect_tables}"),
DBI::dbGetQuery, conn = conn)
library(glue)
sapply(glue("SELECT COUNT(*) FROM {expect_tables}"),
DBI::dbGetQuery, conn = conn)
setdiff(expect_tables, found_tables)
sapply(glue("SELECT COUNT(*) FROM {found_tables}"),
DBI::dbGetQuery, conn = conn)
n_col <- sapply(glue("SELECT * FROM {expect_tables} LIMIT 5"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem) %>% dim %>% .[2]) %>%
setNames(nm = expect_tables)
library(magrittr)
sapply(glue("SELECT COUNT(*) FROM {found_tables}"),
DBI::dbGetQuery, conn = conn)
n_col <- sapply(glue("SELECT * FROM {expect_tables} LIMIT 5"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem) %>% dim %>% .[2]) %>%
setNames(nm = expect_tables)
n_col <- sapply(glue("SELECT * FROM {found_tables} LIMIT 5"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem) %>% dim %>% .[2]) %>%
setNames(nm = expect_tables)
DBI::dbGetQuery(conn = conn, statement = statem) %>% dim %>% .[2]) %>%
setNames(nm = found_tables)
file.size(db_name)
lapply(glue("SELECT * FROM {dbListTables(conn)}"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem)) %>%
object.size
library(DBI)
dbListTables
lapply(glue("SELECT * FROM {dbListTables(conn)}"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem)) %>%
object.size
db_content <- lapply(glue("SELECT * FROM {dbListTables(conn)}"), FUN = function(statem){
DBI::dbGetQuery(conn = conn, statement = statem)
}) %>%
setNames(nm = dbListTables(conn))
db_name_tmp <- db_name %>%
gsub(pattern = ".db", replacement = "_copy2_tmp.db")
conn_tmp <- DBI::dbConnect(RSQLite::SQLite(), db_name_tmp)
name <- names(db_content)[1]
val <- db_content[[name]]
for(name in names(db_content)){
DBI::dbWriteTable(
conn = conn_tmp,
name = names(db_content)[1],
value = db_content[[name]],
overwrite = TRUE
)
}
fetch_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time),
row.names = TRUE
)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
id_time <- rownames(fetch_time)
id_jobid <- fetch_jobid$job_id
setdiff(id_time, id_jobid)
setdiff(id_jobid, id_time)
id_jobid
id_time
found_tables
setdiff(id_jobid, id_time)
setdiff(id_time, id_jobid)
id_jobid %>% {.[grepl(pattern = "MASTERCARD", .)]}
id_jobid %>% {.[grepl(pattern = "HELLA", .)]}
setdiff(id_time, id_jobid)
id_jobid %>% {.[grepl(pattern = "SALZGITTER", .)]}
id_time %>% {.[grepl(pattern = "SALZGITTER", .)]}
fetch_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time),
row.names = TRUE
)
fetch_time %>% head
DBI::dbDisconnect(conn = conn)
SteveAI::run()
library(DBI)
library(magrittr)
library(glue)
setwd("~")
db_name <- "rvest_scraper.db"
db_exists <- file.exists(db_name)
if(db_exists){
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
}
# 5.
found_tables <- DBI::dbListTables(conn)
expect_tables <- c("RVEST_MULTI_JOBS",  "RVEST_MULTI_TIME",  "RVEST_SINGLE_JOBS", "RVEST_SINGLE_TIME")
unknown_tables <- setdiff(found_tables, expect_tables)
unknown_tables
missing_tables <- setdiff(expect_tables, found_tables)
missing_tables
sapply(glue("SELECT COUNT(*) FROM {found_tables}"),
DBI::dbGetQuery, conn = conn)
n_col <- sapply(glue("SELECT * FROM {found_tables} LIMIT 5"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem) %>% dim %>% .[2]) %>%
setNames(nm = found_tables)
file.size(db_name)
lapply(glue("SELECT * FROM {dbListTables(conn)}"), FUN = function(statem)
DBI::dbGetQuery(conn = conn, statement = statem)) %>%
object.size
db_content <- lapply(glue("SELECT * FROM {dbListTables(conn)}"), FUN = function(statem){
DBI::dbGetQuery(conn = conn, statement = statem)
}) %>%
setNames(nm = dbListTables(conn))
db_name_tmp <- db_name %>%
gsub(pattern = ".db", replacement = "_copy2_tmp.db")
conn_tmp <- DBI::dbConnect(RSQLite::SQLite(), db_name_tmp)
name <- names(db_content)[1]
val <- db_content[[name]]
for(name in names(db_content)){
DBI::dbWriteTable(
conn = conn_tmp,
name = names(db_content)[1],
value = db_content[[name]],
overwrite = TRUE
)
}
fetch_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time),
row.names = TRUE
)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
id_time <- rownames(fetch_time)
id_jobid <- fetch_jobid$job_id
setdiff(id_time, id_jobid)
setdiff(id_jobid, id_time)
id_jobid %>% {.[grepl(pattern = "SALZGITTER", .)]}
id_time %>% {.[grepl(pattern = "SALZGITTER", .)]}
id_time %>% head
id_time %>% nchar %>% table
# shortest names
xx <- nchar(id_time)
?min
order(xx, decreasing = FALSE) %>% head()
f <- order(xx, decreasing = FALSE) %>% head()
xx[f]
id_time[f]
?head
f <- order(xx, decreasing = FALSE) %>% head(n = 10)
id_time[f]
f <- order(xx, decreasing = FALSE) %>% head(n = 15)
id_time[f]
# longest names
xx <- nchar(id_time)
f <- order(xx, decreasing = TRUE) %>% head(n = 15)
id_time[f]
library(shiny)
library(shinydashboard)
library(shinyFiles)
library(shinyWidgets)
ui <- fluidPage(
shinyDirButton("preinfolder", "Choose a folder" ,
title = "Please select a folder:",
buttonType = "default", class = NULL,
icon = icon("folder", lib = "font-awesome"), multiple = TRUE),
textOutput("prein_txt_file")
)
server <- function(input, output, session) {
volumes = getVolumes()()
observe({
shinyDirChoose(input, "preinfolder", roots = volumes, session = session)
if(!is.null(input$preinfolder)){
# browser()
preinfolder_selected<-parseDirPath(volumes, input$preinfolder)
output$prein_txt_file <- renderText(preinfolder_selected)
}})
}
library(shiny)
library(shinydashboard)
library(shinyFiles)
library(shinyWidgets)
ui <- fluidPage(
shinyDirButton("preinfolder", "Choose a folder" ,
title = "Please select a folder:",
buttonType = "default", class = NULL,
icon = icon("folder", lib = "font-awesome")),
textOutput("prein_txt_file")
)
server <- function(input, output, session) {
volumes = getVolumes()()
observe({
shinyDirChoose(input, "preinfolder", roots = volumes, session = session)
if(!is.null(input$preinfolder)){
# browser()
preinfolder_selected<-parseDirPath(volumes, input$preinfolder)
output$prein_txt_file <- renderText(preinfolder_selected)
}})
}
shinyApp(ui, server)
?shinyDirButton
