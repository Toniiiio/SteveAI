observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_items)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_items <- setdiff(global$error_items, input$error_item)
print(length(unlist(global$error_items)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(input$get_xpath, {
print(input$target_text)
print(global$doc)
txt <- getXPathByText(doc = global$doc, text = input$target_text)
print(txt)
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
})
observeEvent(input$use_xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
})
output$curl <- renderPrint(global$items)
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
})
output$curl <- renderPrint(global$curl_output)
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
})
output$curl <- renderPrint(global$domain_output)
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_items = error_items)
output$items <- renderUI({
print(length(global$error_items))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_items,
selected = global$error_items[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_items)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_items <- setdiff(global$error_items, input$error_item)
print(length(unlist(global$error_items)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(input$get_xpath, {
print(input$target_text)
print(global$doc)
txt <- getXPathByText(doc = global$doc, text = input$target_text)
print(txt)
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
})
observeEvent(input$use_xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- "global$items"
})
output$curl <- renderPrint(global$xpath_output)
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
})
output$curl <- renderPrint(global$curl_output)
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
})
output$curl <- renderPrint(global$domain_output)
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_items = error_items, xpath_output = NULL)
output$items <- renderUI({
print(length(global$error_items))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_items,
selected = global$error_items[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_items)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_items <- setdiff(global$error_items, input$error_item)
print(length(unlist(global$error_items)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(input$get_xpath, {
print(input$target_text)
print(global$doc)
txt <- getXPathByText(doc = global$doc, text = input$target_text)
print(txt)
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
})
observeEvent(input$use_xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- "global$items"
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
})
output$curl <- renderPrint(global$curl_output)
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
})
output$curl <- renderPrint(global$domain_output)
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_items = error_items, xpath_output = NULL)
output$items <- renderUI({
print(length(global$error_items))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_items,
selected = global$error_items[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_items)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_items <- setdiff(global$error_items, input$error_item)
print(length(unlist(global$error_items)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(input$get_xpath, {
print(input$target_text)
print(global$doc)
txt <- getXPathByText(doc = global$doc, text = input$target_text)
print(txt)
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
})
observeEvent(input$use_xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- global$items
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
})
output$curl <- renderPrint(global$curl_output)
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
})
output$curl <- renderPrint(global$domain_output)
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
library(tryCatchLog)
library(rvest)
library(glue)
library(futile.logger)
library(magrittr)
library(DBI)
source("SteveAI/R/logging.R")
source("R/logging.R")
source('~/SteveAI/R/scheduled_Scraping_rvest.R', echo=TRUE)
library(magrittr)
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
fetch_jobid %>% head
fetch_job_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time)
)
fetch_job_time %>% head
log_path <- "~"
UC_Name = "rvest_single"
logger_name = "sivis"
file_name <- file.path(log_path, paste0(UC_Name, "_", Sys.Date(), ".log"))
initialize(logger_name = logger_name, trennzeichen = "___", log_path = "~", UC_Name = "rvest_single", file_name)
flog.info("test", name = logger_name)
log_Data <- file_name %>%
readLines
log_Data
date_today = Sys.Date()
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 4
for(get_nr in seq(SteveAI::rvestScraper)){
print(get_nr)
target_name <- names(SteveAI::rvestScraper)[get_nr]
scraper = SteveAI::rvestScraper[[get_nr]]
response <- tryCatchLog(
expr = scraper$url %>% httr::GET(),
error = function(e){
scrape_log_error(
target_name = target_name,
msg = e,
url = scraper$url,
logger_name = logger_name
)
}
)
file_Name <- glue("{names(SteveAI::rvestScraper)[get_nr]}_{date_today}.RData")
save(response, file = glue("{folder_name}/{file_Name}"))
}
responses <- list.files(folder_name)
nms <- responses %>%
sapply(FUN = function(response){
strsplit(x = response, split = "_") %>% unlist %>% .[1]
})
if(!length(nms)) stop("Did not find any downloaded files.")
file.copy(from = "~/rvest_scraper.db", to = glue::glue("~/rvest_scraper_{date_today}_BACKUP.db"))
nr <- 1
#names(SteveAI::rvestScraper)
for(nr in seq(SteveAI::rvestScraper)){
print(nr)
scraper <- SteveAI::rvestScraper[[nr]]
name <- names(SteveAI::rvestScraper)[nr]
file_Name <- which(name == nms) %>% names
if(!length(file_Name)){
warning("no such file with response.")
next
}
# loading variable: response here
load(file.path(folder_name, file_Name))
start <- Sys.time()
print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper, name = name, date_today = date_today),
error = function(e){
print(e)
name <- names(SteveAI::rvestScraper)[nr]
url <- SteveAI::rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
if(is.null(data_raw[[name]]) | length(data_raw[[name]]) < 2) next
# data_raw
# has_Id <- FALSE #names(data_raw[[name]])
# if(has_Id){
#
# }else{
#
#   # build id - take all non-empty columns that do not have blacklist items
#   apply(data_raw, 2, nchar)
#
# }
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
# fileNameJobDesc = paste0("dataRvest/JobDescLinks_", name, "_", date_today, ".csv")
# write.csv2(x = data_raw[[name]], file = fileNameJobDesc, row.names = FALSE)
# write.table(
#   data.frame(
#     name = name,
#     duration = scrapeDuration
#   ),
#   file = durationFileName,
#   append = TRUE,
#   row.names = FALSE,
#   col.names = FALSE
# )
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
url = scraper$url
if(!is.null(out)){
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = url,
logger_name = logger_name,
date_today = date_today
)
}else{
warning(glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."))
scrape_log_warn(
target_name = name,
url = url,
msg = glue::glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."),
logger_name = logger_name
)
}
}
