print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper, name = name),
error = function(e){
print(e)
name <- names(SteveAI::rvestScraper)[nr]
url <- SteveAI::rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
if(is.null(data_raw[[name]]) | length(data_raw[[name]]) < 2) next
# data_raw
# has_Id <- FALSE #names(data_raw[[name]])
# if(has_Id){
#
# }else{
#
#   # build id - take all non-empty columns that do not have blacklist items
#   apply(data_raw, 2, nchar)
#
# }
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
# fileNameJobDesc = paste0("dataRvest/JobDescLinks_", name, "_", Sys.Date(), ".csv")
# write.csv2(x = data_raw[[name]], file = fileNameJobDesc, row.names = FALSE)
# write.table(
#   data.frame(
#     name = name,
#     duration = scrapeDuration
#   ),
#   file = durationFileName,
#   append = TRUE,
#   row.names = FALSE,
#   col.names = FALSE
# )
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
if(!is.null(out)){
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = scraper$url,
logger_name = logger_name
)
}else{
warning(glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."))
scrape_log_warn(
target_name = name,
url = url,
msg = glue::glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."),
logger_name = logger_name
)
}
}
end <- Sys.time()
end - start
}
### repair SteveAI::rvestScraper
# for(nr in seq(SteveAI::rvestScraper)){
#   nms <- names(SteveAI::rvestScraper[[nr]])
#   idx <- nms == "XPath"
#   if(sum(idx)){
#     print(idx)
#     nms[idx] <- "jobNameXpath"
#     SteveAI::rvestScraper[[nr]] <- lapply(
#       SteveAI::rvestScraper[[nr]],
#       FUN = function(x){
#         ifelse(test = typeof(x) == "closure", yes = x, no = as.character(x))
#       }
#     )
#     names(SteveAI::rvestScraper[[nr]]) <- nms
#   }
# }
# save(SteveAI::rvestScraper, file = "scraper_rvest.RData")
# new_jobNames <- setdiff(rownames(new_jobs_time), rownames(fetch))
# to_add <- matrix(0, ncol = dim(fetch)[2], nrow = length(new_jobNames)) %>%
#   data.frame(row.names = new_jobNames) %>%
#   setNames(colnames(fetch))
#
# old_data <- rbind(fetch, to_add)
# old_data$title <- rownames(old_data)
# to_db <- merge(
#   x = old_data,
#   y = data.frame(title = rownames(new_jobs_time), count = new_jobs_time[[1]]),
#   by = "title",
#   all.x = TRUE
# )
# to_db$count[is.na(to_db$count)] <- 0
#
# rownames(to_db) <- to_db$title
# to_db$title <- NULL
# old_data$title <- NULL
# colnames(to_db) <- c(colnames(old_data), Sys.Date())
#
# if(need_update){
#
#   DBI::dbWriteTable(
#     conn = conn,
#     name = target_table_time,
#     value = to_db,
#     overwrite = TRUE,
#     row.names= TRUE
#   )
#
# }
print(Sys.time())
data_raw <- list()
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{Sys.Date()}.csv")
folder_name <- glue("response_raw/{Sys.Date()}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 1
#
for(get_nr in seq(SteveAI::rvestScraper)){
print(get_nr)
target_name <- names(SteveAI::rvestScraper)[get_nr]
scraper = SteveAI::rvestScraper[[get_nr]]
response <- tryCatchLog(
expr = scraper$url %>% httr::GET(),
error = function(e){
scrape_log_error(
target_name = target_name,
msg = e,
url = scraper$url,
logger_name = logger_name
)
}
)
file_Name <- glue("{names(SteveAI::rvestScraper)[get_nr]}_{Sys.Date()}.RData")
save(response, file = glue("{folder_name}/{file_Name}"))
}
add_http <- function(url, comp_data){
domain <- urltools::domain(comp_data$url)
domain_short <- gsub(domain, pattern = "www.", replacement = "")
# requirement: "https://www.clark.de/de/jobs?gh_jid=4924568002" + "boards.greenhouse.io"
is_sub_url <- (!grepl(domain_short, url)  & !grepl(pattern = ".de|.com|.net", url, perl = TRUE)) |
substring(url, first = 1, last = 1) == "/"
if(is_sub_url){
url <- paste0(
"https://",
domain,
"/",
url
)
}
# todo: very dirty
# reason: https://www.cofinpro.de//karriere/stellenanzeige/manager-mit-schwerpunkt-foerderbanken leads to an error
url <- gsub(x = url, pattern = "//", replacement = "/", fixed = TRUE)
url <- gsub(x = url, pattern = "http:/", replacement = "http://", fixed = TRUE)
gsub(x = url, pattern = "https:/", replacement = "https://", fixed = TRUE)
}
link_to_article <- function(links){
read_html_txt <- function(url){
tryCatch(htm2txt::gettxt(url), error = function(e){
warning(paste0(url, "fails with", e))
return("ERROR")
})
}
job_descs_raw <- lapply(links, read_html_txt)
job_descs <- job_descs_raw[!duplicated(job_descs_raw) & job_descs_raw != "ERROR"]
if(length(job_descs) < 2){
warning("Not enough article found can not exclude header and footer.")
return(job_descs_raw)
}
iter <- min(20, length(job_descs)) - 1
end_of_starts <- rep(NA, iter - 1)
for(nr in 2:(iter + 1)){
end_of_starts[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = FALSE)
}
end_of_start <- min(end_of_starts)
iter <- min(20, length(job_descs)) - 1
start_of_endings <- rep(NA, iter - 1)
for(nr in 2:(iter + 1)){
start_of_endings[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = TRUE)[1]
}
start_of_ending <- max(start_of_endings)
cut_start <- substring(job_descs[[1]], 1, end_of_start)
cut_end <- substring(job_descs[[1]], start_of_ending[1], nchar(job_descs[[1]]))
job_descs_out <- rep(NA, length(job_descs_raw))
for(nr in seq(job_descs_raw)){
print(nr)
job_descs_out[nr] <- extract_article(text_raw = job_descs_raw[nr], cut1 = cut_start, cut2 = cut_end)
}
return(job_descs_out)
}
library(glue)
library(magrittr)
library(rvest)
setwd("~")
source("SteveAI/R/jobLinks.R")
# 1. read all job data
# 2. Filter all that have links
# 3. Filter for the ones that arent scraped yet.
# 4. Group by company to identify the format and what to remove from the job desc page.
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
data <- DBI::dbGetQuery(
conn = conn,
statement = glue("SELECT * FROM {target_table_job}")
)
keep <- which(!is.na(data$links))
length(keep)
comp_nr <- 3
with_links <- data[keep, ]
all_comps <- with_links$comp %>% unique
all_job_descs <- vector("list", length(all_comps))
#seq(all_comps)
# 10 -> error, 15
comp_nr <- 24
for(comp_nr in 1:3){
comp_name <- all_comps[comp_nr]
print(comp_name)
comp_data <- SteveAI::rvestScraper[[comp_name]]
doc <- comp_data$url %>% read_html()
links <- doc %>%
html_nodes(xpath = comp_data$href) %>%
html_attr(name = "href")
links <- sapply(links, add_http, comp_data = comp_data, USE.NAMES = FALSE)
job_descs <- link_to_article(links)
all_job_descs[[comp_nr]] <- data.frame(
links = links,
job_descs = job_descs,
date = Sys.Date()
)
}
comp_nr
comp_name <- all_comps[comp_nr]
print(comp_name)
comp_data <- SteveAI::rvestScraper[[comp_name]]
doc <- comp_data$url %>% read_html()
links <- doc %>%
html_nodes(xpath = comp_data$href) %>%
html_attr(name = "href")
links <- sapply(links, add_http, comp_data = comp_data, USE.NAMES = FALSE)
job_descs <- link_to_article(links)
links
read_html_txt <- function(url){
tryCatch(htm2txt::gettxt(url), error = function(e){
warning(paste0(url, "fails with", e))
return("ERROR")
})
}
job_descs_raw <- lapply(links, read_html_txt)
job_descs <- job_descs_raw[!duplicated(job_descs_raw) & job_descs_raw != "ERROR"]
if(length(job_descs) < 2){
warning("Not enough article found can not exclude header and footer.")
return(job_descs_raw)
}
iter <- min(20, length(job_descs)) - 1
end_of_starts <- rep(NA, iter - 1)
reverse = FALSE
text2 = job_descs[[nr]]
text1 = job_descs[[1]]
if(text1 == text2){
warning("text1 equals text2")
}
text2 = job_descs[[nr]]
nr <- 2
text2 = job_descs[[nr]]
text1 = job_descs[[1]]
if(text1 == text2){
warning("text1 equals text2")
}
if(reverse){
text1 <- text1 %>%
strsplit(split = "") %>%
"[["(1) %>%
rev() %>%
paste(collapse = "")
text2 <- text2 %>%
strsplit(split = "") %>%
"[["(1) %>%
rev() %>%
paste(collapse = "")
}
max <- min(nchar(text1), nchar(text2))
seq <- seq(10, max, by = 70)
same <- TRUE
nr <- 1
while(same & nr <= length(seq)){
dist <- adist(substr(text1, 1, seq[nr]), substr(text2, 1, seq[nr])) / seq[nr]
same <- dist[1, 1] == 0
print(same)
nr <- nr + 1
}
if(nr == 2 | (nr + 1) == length(seq)){
warning("no match found - maybe they start or end with same text.")
return(0)
}
pos <- seq[nr]
print(pos)
dist <- adist(substr(text1, 1, pos), substr(text2, 1, pos)) / pos
while(!same){
print(pos)
dist <- adist(substr(text1, 1, pos), substr(text2, 1, pos)) / pos
same <- dist[1, 1] == 0
pos <- pos - 1
}
text2
pos
seq
# low performance (speed) if adist for long strings
lower_bound <- max(1, pos - 100)
dist <- adist(substr(text1, lower_bound, substr(text2, lower_bound, pos))
same <- dist[1, 1] == 0
pos <- pos - 1
}
if(reverse) pos = c(nchar(text1) - pos, nchar(text2) - pos)
return(pos)
}
#
#
# xx <- findEnd(text1, text2, reverse = TRUE)
#
# cut1 <- substring(text1, 1, pos)
# cut2 <- substring(text1, xx[1], nchar(text1))
#
# text_raw <- xxx[[25]]
#
remove_header_footer <- function(text_raw, cut, dist = 0.3, header = TRUE){
if(!nchar(cut)){
return(text_raw)
}
str_len <- nchar(cut)
s <- 1000
if(str_len > s) cut <- substring(cut, 1, s)
m <- aregexec(cut, text_raw, max.distance = dist, fixed = TRUE)
if(m[[1]][1] == -1){
warning(paste0("No match for ", cut))
xx
return(text_raw)
}
if(header){
replace <- substring(text_raw, first = m[[1]][1], last = m[[1]][1] + str_len) %>%
unlist %>%
.[1]
}else{
replace <- substring(text_raw, first = m[[1]][1], last = str_len) %>%
unlist %>%
.[1]
}
out <- text_raw %>%
gsub(pattern = replace, replacement = "", fixed = TRUE)
return(out)
}
extract_article <- function(text_raw, cut1, cut2){
if(text_raw[[1]] == "ERROR") return(text_raw[[1]])
text_raw <- remove_header_footer(text_raw, cut1) # remove header
text_raw <- remove_header_footer(text_raw, cut2) # remove footer
return(text_raw)
}
dist <- adist(substr(text1, lower_bound, pos), substr(text2, lower_bound, pos))
same <- dist[1, 1] == 0
same
pos <- seq[nr]
while(!same){
print(pos)
# low performance (speed) if adist for long strings
lower_bound <- max(1, pos - 100)
dist <- adist(substr(text1, lower_bound, pos), substr(text2, lower_bound, pos))
same <- dist[1, 1] == 0
pos <- pos - 1
}
if(reverse) pos = c(nchar(text1) - pos, nchar(text2) - pos)
pos
findEnd <- function(text1, text2, reverse = FALSE){
if(text1 == text2){
warning("text1 equals text2")
}
if(reverse){
text1 <- text1 %>%
strsplit(split = "") %>%
"[["(1) %>%
rev() %>%
paste(collapse = "")
text2 <- text2 %>%
strsplit(split = "") %>%
"[["(1) %>%
rev() %>%
paste(collapse = "")
}
max <- min(nchar(text1), nchar(text2))
seq <- seq(10, max, by = 70)
same <- TRUE
nr <- 1
while(same & nr <= length(seq)){
dist <- adist(substr(text1, 1, seq[nr]), substr(text2, 1, seq[nr])) / seq[nr]
same <- dist[1, 1] == 0
print(same)
nr <- nr + 1
}
if(nr == 2 | (nr + 1) == length(seq)){
warning("no match found - maybe they start or end with same text.")
return(0)
}
pos <- seq[nr]
while(!same){
print(pos)
# low performance (speed) if adist for long strings
lower_bound <- max(1, pos - 100)
dist <- adist(substr(text1, lower_bound, pos), substr(text2, lower_bound, pos))
same <- dist[1, 1] == 0
pos <- pos - 1
}
if(reverse) pos = c(nchar(text1) - pos, nchar(text2) - pos)
return(pos)
}
for(comp_nr in 1:3){
comp_name <- all_comps[comp_nr]
print(comp_name)
comp_data <- SteveAI::rvestScraper[[comp_name]]
doc <- comp_data$url %>% read_html()
links <- doc %>%
html_nodes(xpath = comp_data$href) %>%
html_attr(name = "href")
links <- sapply(links, add_http, comp_data = comp_data, USE.NAMES = FALSE)
job_descs <- link_to_article(links)
all_job_descs[[comp_nr]] <- data.frame(
links = links,
job_descs = job_descs,
date = Sys.Date()
)
}
comp_nr
read_html_txt <- function(url){
tryCatch(htm2txt::gettxt(url), error = function(e){
warning(paste0(url, "fails with", e))
return("ERROR")
})
}
job_descs_raw <- lapply(links, read_html_txt)
job_descs <- job_descs_raw[!duplicated(job_descs_raw) & job_descs_raw != "ERROR"]
if(length(job_descs) < 2){
warning("Not enough article found can not exclude header and footer.")
return(job_descs_raw)
}
iter <- min(20, length(job_descs)) - 1
end_of_starts <- rep(NA, iter - 1)
nr <- 2
for(nr in 2:(iter + 1)){
end_of_starts[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = FALSE)
}
end_of_start <- min(end_of_starts)
iter <- min(20, length(job_descs)) - 1
start_of_endings <- rep(NA, iter - 1)
for(nr in 2:(iter + 1)){
start_of_endings[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = TRUE)[1]
}
start_of_ending <- max(start_of_endings)
cut_start <- substring(job_descs[[1]], 1, end_of_start)
cut_end <- substring(job_descs[[1]], start_of_ending[1], nchar(job_descs[[1]]))
job_descs_out <- rep(NA, length(job_descs_raw))
nr <- 200
end_of_starts[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = FALSE)
iter <- min(20, length(job_descs)) - 1
end_of_starts <- rep(NA, iter - 1)
iter
iter <- min(20, length(job_descs)) - 1
end_of_starts <- rep(NA, iter - 1)
nr <- 200
for(nr in 2:(iter + 1)){
end_of_starts[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = FALSE)
}
end_of_start <- min(end_of_starts)
iter <- min(20, length(job_descs)) - 1
start_of_endings <- rep(NA, iter - 1)
iter
iter <- min(20, length(job_descs)) - 1
start_of_endings <- rep(NA, iter - 1)
for(nr in 2:(iter + 1)){
start_of_endings[nr - 1] <- findEnd(text1 = job_descs[[1]], text2 = job_descs[[nr]], reverse = TRUE)[1]
}
start_of_ending <- max(start_of_endings)
cut_start <- substring(job_descs[[1]], 1, end_of_start)
cut_end <- substring(job_descs[[1]], start_of_ending[1], nchar(job_descs[[1]]))
job_descs_out <- rep(NA, length(job_descs_raw))
seq(job_descs_raw)
nr <- 199
print(nr)
job_descs_out[nr] <- extract_article(text_raw = job_descs_raw[nr], cut1 = cut_start, cut2 = cut_end)
job_descs_out[nr]
job_descs_out[nr] %>% cat
for(nr in seq(job_descs_raw)){
print(nr)
job_descs_out[nr] <- extract_article(text_raw = job_descs_raw[nr], cut1 = cut_start, cut2 = cut_end)
}
all_job_descs[[comp_nr]] <- data.frame(
links = links,
job_descs = job_descs,
date = Sys.Date()
)
all_job_descs[[comp_nr]]
all_job_descs[[comp_nr]]$job_descs..Menü.schließen.n.n..Über.uns.n..Standorte.n.n..Augsburg.n..Berlin.n..Bremen.n..Chemnitz.n..Dortmund.n..Dresden.n..Düsseldorf.n..Essen.n..Frankfurt.n..Freiburg.n..Halle..Saale..n..Hamburg.n..Hannover.n..Köln.n..Leipzig.n..Magdeburg.n..Mainz.n..München.n..Münster.n.n..Aufgabenbereiche.n.n..IT...Web.Development.n..Produktmanagement.n..Kundenberatung...Service.n..Marketing...Kommunikation.n..Zentrale.Funktionen.n.nJob.finden.n.nTeamleiter.Frontend.Entwicklung..m.w.d..Kredite.n.nWir.stellen.weiterhin.ein..n.nUnser.Recruiting.Versprechen.während.COVID.19.n.n..CHECK24.wächst.weiter..Wir.sind.auch.jetzt.auf.der.Suche.nach.talentierten..neuen.Mitarbeitern.n..Flexibilität.bei.Vorstellungsgesprächen..Unsere.Gespräche.finden.via.Videokonferenz.oder.vereinzelt.persönlich.statt.n..Sicheres.Onboarding..Persönliche.und.flexible.Einarbeitung.durch.unsere.erfahrenen.Kollegen.unter.Einhaltung.des.Mindestabstandes.und.notwendiger.Hygienevorschriften.n.nMünchen.n.nWir.sind.Pionier.und.langjähriger.Marktführer.für.Online.Kreditvergleiche.mit.Millionen.von.Kunden..Dich.erwarten.modernste.Technologien.wie.Kubernetes..Elasticsearch.und.Grafana.in.einem.agilen.Umfeld..Wir.reden.nicht.nur.über.CI.CD..wir.leben.es.mit.typischerweise.über.10.Feature.Releases.am.Tag..n..nDafür.brauchen.wir.Dich..um.mit.uns.gemeinsam.die.Software.Plattform.zu.gestalten.und.weiter.zu.entwickeln..Wir.setzen.auf.Technologien.wie.Microservices.mit.Spring.Boot..Angular.und.React..nWir.freuen.uns.auf.Deine.Bewerbung.als.Teamleiter.Frontend.Entwicklung..m.w.d..bei.der.CHECK24.Vergleichsportal.Finanzen.GmbH..nWie.Spaß.bei.der.Arbeit.und.innovative.Weiterentwicklung.unseres.Kreditvergleichs.aussieht..kannst.Du.direkt.in.unserem.Video.sehen..n.nZu.Deinen.Aufgaben.zählen.n.n..Du.führst.Dein.Team.fachlich.sowie.disziplinarisch.n..Du.förderst.die.technische.und.persönliche.Weiterentwicklung.Deiner.Mitarbeiter.n..Du.arbeitest.aktiv.mit.Deinem.Team.bei.der.Umsetzung.von.Stories.n..Du.gestaltest.unsere.IT.Strategie.aktiv.mit.n..Du.trägst.mit.Deinem.Team.die.Verantwortung.für.den.Betrieb.den.langfristigen.Erfolg.der.Plattform.n.nWas.Du.mitbringst.n.n..Eine.Leidenschaft.für.moderne..performante.Webanwendungen.n..Den.Ehrgeiz..Deine.persönlichen.Fähigkeiten.ständig.weiter.zu.entwickeln.n..Die.Fähigkeit..immer.das.Ziel.vor.Augen.zu.haben.und.aus.Fehlern.zu.lernen.n..Überzeugende.Kenntnisse.von.HTML5..CSS.SCSS..JavaScript..TypeScript.n..Überzeugende.Kenntnisse.von.Angular.sowie.Node.js.n..Gute.Kenntnisse.von.Microservices.mit.REST.APIs.n..Du.konntest.bereits.erste.Erfahrungen.in.Führungsrollen.sammeln.n..Verhandlungssichere.Deutsch..und.Englischkenntnisse.n.nWas.wir.Dir.bieten.n.n..Mit.hunderten.Entwicklern..in.kleinen.agilen.Teams..sind.wir.mehr.als.eine.Website.n..Mit.mehr.als.10.Jahren.Erfolg.am.Markt.bieten.wir.Dir.die.Stabilität.eines.etablierten.Online.Unternehmens...und.stellen.unbefristet.in.Festanstellung.ein.n..Steile.Karriere.Chancen.durch.kontinuierliches.Wachstum...schlage.individuell.den.Weg.zum.Fachexperten.ein.oder.entwickle.Dich.bei.uns.zur.Führungskraft.n..Individuelle.Entwicklungs..und.Weiterbildungsmöglichkeiten.dank.einem.breiten.Trainingskatalog.und.einer.spezialisierten.Personalentwicklung.für.Deinen.Karrierepfad.n..Gutscheine.für.zahlreiche.CHECK24.Produkte.und.eine.bezuschusste.betriebliche.Altersversorgung.n..Extras.wie.Getränke.und.Obst.sind.bei.uns.selbstverständlich..n..Zentraler.Standort.mit.ausgezeichneter.Verkehrsanbindung.und.einem.von.uns.sehr.gutem.bezuschussten.MVV.Ticket.n..Erfolge.feiern.wir.gemeinsam.auf.zahlreichen.Events.n.nBereit.für.CHECK24..n.nBewerben.Sie.sich.jetzt..nÜberzeuge.uns.im.nächsten.Schritt.durch.Deine.Antworten.und.bewirb.Dich.nur.mit.Deinem.Lebenslauf.und.Zeugnissen..nBewerbungen.können.ausschließlich.online.über.unser.Bewerbungsformular.angenommen.werden..n.nBewerben.n.nBewerben.Bewerbungstipps.zum.Standort.n.nKontakt.n.nVerena.Hauke.nverena.hauke.check24.de.n.nTeilen.n.nDieser.Job.bei.CHECK24.könnte.perfekt.zu.Dir.passen..n.nHallo..nSchau.dir.diesen.Job.an.n.__link__...nBewirb.dich..n.n..n..n..n..Impressum.Open.Source.n.nImpressum.n.nBevor.Du.weiter.klickst.n.nIndem.Du.auf..Akzeptieren..klickst..ermöglichst.Du.uns.über.Cookies.das.Nutzungserlebnis.für.alle.User.kontinuierlich.zu.verbessern..Durch.den.Klick.erteilst.Du.uns.Deine.ausdrückliche.Einwilligung..Über..Konfigurieren..kannst.Du.Deine.Einwilligung.individuell.anpassen..dies.ist.auch.nachträglich.jederzeit.im.Bereich.Datenschutz.möglich...Die.zugehörige.Datenschutzhinweise.findest.Du.hier..n.nKonfigurieren.Akzeptieren.n.n..zurück.n.nDeine.Cookie.Präferenzen.verwalten.n.nWähle..welche.Cookies.Du.auf.jobs.check24.de.akzeptierst..n.nFunktionale.Cookies.n.nDiese.Cookies.und.andere.Informationen.sind.für.die.Funktion.unserer.Services.unbedingt.erforderlich..Sie.garantieren..dass.unser.Service.sicher.und.so.wie.von.Dir.gewünscht.funktioniert..Daher.kann.man.sie.nicht.deaktivieren..n.nAnalytische.Cookies.n.nWir.möchten.für.Dich.unseren.Service.so.gut.wie.möglich.machen..Daher.verbessern.wir.unsere.Karrierewebseite.und.Dein.Nutzungserlebnis.stetig..Um.dies.zu.tun..möchten.wir.die.Nutzung.der.Webseite.analysieren.und.in.statistischer.Form.auswerten..n.nMarketing.Cookies.n.nUm.unseren.Service.noch.persönlicher.zu.machen..spielen.wir.mit.Hilfe.dieser.Cookies.und.anderer.Informationen.personalisierte.Empfehlungen.und.Werbung.aus.und.ermöglichen.eine.Interaktion.mit.sozialen.Netzwerken..Die.Cookies.werden.von.uns.und.unseren.Werbepartnern.gesetzt..Dies.ermöglicht.uns.und.unseren.Partnern..den.Nutzern.unseres.Services.personalisierte.Werbung.anzuzeigen..die.auf.einer.website..und.geräteübergreifenden.Analyse.Ihres.Nutzungsverhaltens.basiert..Die.mit.Hilfe.der.Cookies.erhobenen.Daten.können.von.uns.und.unseren.Partnern.mit.Daten.von.anderen.Websites.zusammengeführt.werden..n.nAuswahl.speichern.Alle.akzeptieren.
all_job_descs[[comp_nr]]$job_descs..Menü.schließen.n.n..Über.uns.n..Standorte.n.n..Augsburg.n..Berlin.n..Bremen.n..Chemnitz.n..Dortmund.n..Dresden.n..Düsseldorf.n..Essen.n..Frankfurt.n..Freiburg.n..Halle..Saale..n..Hamburg.n..Hannover.n..Köln.n..Leipzig.n..Magdeburg.n..Mainz.n..München.n..Münster.n.n..Aufgabenbereiche.n.n..IT...Web.Development.n..Produktmanagement.n..Kundenberatung...Service.n..Marketing...Kommunikation.n..Zentrale.Funktionen.n.nJob.finden.n.nTeamleiter.Frontend.Entwicklung..m.w.d..Kredite.n.nWir.stellen.weiterhin.ein..n.nUnser.Recruiting.Versprechen.während.COVID.19.n.n..CHECK24.wächst.weiter..Wir.sind.auch.jetzt.auf.der.Suche.nach.talentierten..neuen.Mitarbeitern.n..Flexibilität.bei.Vorstellungsgesprächen..Unsere.Gespräche.finden.via.Videokonferenz.oder.vereinzelt.persönlich.statt.n..Sicheres.Onboarding..Persönliche.und.flexible.Einarbeitung.durch.unsere.erfahrenen.Kollegen.unter.Einhaltung.des.Mindestabstandes.und.notwendiger.Hygienevorschriften.n.nMünchen.n.nWir.sind.Pionier.und.langjähriger.Marktführer.für.Online.Kreditvergleiche.mit.Millionen.von.Kunden..Dich.erwarten.modernste.Technologien.wie.Kubernetes..Elasticsearch.und.Grafana.in.einem.agilen.Umfeld..Wir.reden.nicht.nur.über.CI.CD..wir.leben.es.mit.typischerweise.über.10.Feature.Releases.am.Tag..n..nDafür.brauchen.wir.Dich..um.mit.uns.gemeinsam.die.Software.Plattform.zu.gestalten.und.weiter.zu.entwickeln..Wir.setzen.auf.Technologien.wie.Microservices.mit.Spring.Boot..Angular.und.React..nWir.freuen.uns.auf.Deine.Bewerbung.als.Teamleiter.Frontend.Entwicklung..m.w.d..bei.der.CHECK24.Vergleichsportal.Finanzen.GmbH..nWie.Spaß.bei.der.Arbeit.und.innovative.Weiterentwicklung.unseres.Kreditvergleichs.aussieht..kannst.Du.direkt.in.unserem.Video.sehen..n.nZu.Deinen.Aufgaben.zählen.n.n..Du.führst.Dein.Team.fachlich.sowie.disziplinarisch.n..Du.förderst.die.technische.und.persönliche.Weiterentwicklung.Deiner.Mitarbeiter.n..Du.arbeitest.aktiv.mit.Deinem.Team.bei.der.Umsetzung.von.Stories.n..Du.gestaltest.unsere.IT.Strategie.aktiv.mit.n..Du.trägst.mit.Deinem.Team.die.Verantwortung.für.den.Betrieb.den.langfristigen.Erfolg.der.Plattform.n.nWas.Du.mitbringst.n.n..Eine.Leidenschaft.für.moderne..performante.Webanwendungen.n..Den.Ehrgeiz..Deine.persönlichen.Fähigkeiten.ständig.weiter.zu.entwickeln.n..Die.Fähigkeit..immer.das.Ziel.vor.Augen.zu.haben.und.aus.Fehlern.zu.lernen.n..Überzeugende.Kenntnisse.von.HTML5..CSS.SCSS..JavaScript..TypeScript.n..Überzeugende.Kenntnisse.von.Angular.sowie.Node.js.n..Gute.Kenntnisse.von.Microservices.mit.REST.APIs.n..Du.konntest.bereits.erste.Erfahrungen.in.Führungsrollen.sammeln.n..Verhandlungssichere.Deutsch..und.Englischkenntnisse.n.nWas.wir.Dir.bieten.n.n..Mit.hunderten.Entwicklern..in.kleinen.agilen.Teams..sind.wir.mehr.als.eine.Website.n..Mit.mehr.als.10.Jahren.Erfolg.am.Markt.bieten.wir.Dir.die.Stabilität.eines.etablierten.Online.Unternehmens...und.stellen.unbefristet.in.Festanstellung.ein.n..Steile.Karriere.Chancen.durch.kontinuierliches.Wachstum...schlage.individuell.den.Weg.zum.Fachexperten.ein.oder.entwickle.Dich.bei.uns.zur.Führungskraft.n..Individuelle.Entwicklungs..und.Weiterbildungsmöglichkeiten.dank.einem.breiten.Trainingskatalog.und.einer.spezialisierten.Personalentwicklung.für.Deinen.Karrierepfad.n..Gutscheine.für.zahlreiche.CHECK24.Produkte.und.eine.bezuschusste.betriebliche.Altersversorgung.n..Extras.wie.Getränke.und.Obst.sind.bei.uns.selbstverständlich..n..Zentraler.Standort.mit.ausgezeichneter.Verkehrsanbindung.und.einem.von.uns.sehr.gutem.bezuschussten.MVV.Ticket.n..Erfolge.feiern.wir.gemeinsam.auf.zahlreichen.Events.n.nBereit.für.CHECK24..n.nBewerben.Sie.sich.jetzt..nÜberzeuge.uns.im.nächsten.Schritt.durch.Deine.Antworten.und.bewirb.Dich.nur.mit.Deinem.Lebenslauf.und.Zeugnissen..nBewerbungen.können.ausschließlich.online.über.unser.Bewerbungsformular.angenommen.werden..n.nBewerben.n.nBewerben.Bewerbungstipps.zum.Standort.n.nKontakt.n.nVerena.Hauke.nverena.hauke.check24.de.n.nTeilen.n.nDieser.Job.bei.CHECK24.könnte.perfekt.zu.Dir.passen..n.nHallo..nSchau.dir.diesen.Job.an.n.__link__...nBewirb.dich..n.n..n..n..n..Impressum.Open.Source.n.nImpressum.n.nBevor.Du.weiter.klickst.n.nIndem.Du.auf..Akzeptieren..klickst..ermöglichst.Du.uns.über.Cookies.das.Nutzungserlebnis.für.alle.User.kontinuierlich.zu.verbessern..Durch.den.Klick.erteilst.Du.uns.Deine.ausdrückliche.Einwilligung..Über..Konfigurieren..kannst.Du.Deine.Einwilligung.individuell.anpassen..dies.ist.auch.nachträglich.jederzeit.im.Bereich.Datenschutz.möglich...Die.zugehörige.Datenschutzhinweise.findest.Du.hier..n.nKonfigurieren.Akzeptieren.n.n..zurück.n.nDeine.Cookie.Präferenzen.verwalten.n.nWähle..welche.Cookies.Du.auf.jobs.check24.de.akzeptierst..n.nFunktionale.Cookies.n.nDiese.Cookies.und.andere.Informationen.sind.für.die.Funktion.unserer.Services.unbedingt.erforderlich..Sie.garantieren..dass.unser.Service.sicher.und.so.wie.von.Dir.gewünscht.funktioniert..Daher.kann.man.sie.nicht.deaktivieren..n.nAnalytische.Cookies.n.nWir.möchten.für.Dich.unseren.Service.so.gut.wie.möglich.machen..Daher.verbessern.wir.unsere.Karrierewebseite.und.Dein.Nutzungserlebnis.stetig..Um.dies.zu.tun..möchten.wir.die.Nutzung.der.Webseite.analysieren.und.in.statistischer.Form.auswerten..n.nMarketing.Cookies.n.nUm.unseren.Service.noch.persönlicher.zu.machen..spielen.wir.mit.Hilfe.dieser.Cookies.und.anderer.Informationen.personalisierte.Empfehlungen.und.Werbung.aus.und.ermöglichen.eine.Interaktion.mit.sozialen.Netzwerken..Die.Cookies.werden.von.uns.und.unseren.Werbepartnern.gesetzt..Dies.ermöglicht.uns.und.unseren.Partnern..den.Nutzern.unseres.Services.personalisierte.Werbung.anzuzeigen..die.auf.einer.website..und.geräteübergreifenden.Analyse.Ihres.Nutzungsverhaltens.basiert..Die.mit.Hilfe.der.Cookies.erhobenen.Daten.können.von.uns.und.unseren.Partnern.mit.Daten.von.anderen.Websites.zusammengeführt.werden..n.nAuswahl.speichern.Alle.akzeptieren. %>% cat
all_job_descs[[1]]$job_descs[2]
all_job_descs[[1]]$job_descs[2] %>% cat
all_job_descs[[2]]$job_descs[1] %>% cat
all_job_descs[[3]]$job_descs[1] %>% cat
all_comps
all_comps
