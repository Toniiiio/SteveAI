)
)
)
)
)
})
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
keep <- which((error_items[, error_terms] %>% rowSums()) > 0)
error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_comps = error_comps, xpath_output = NULL)
observe({
keep <- which((error_items[, input$error_terms] %>% rowSums()) > 0)
global$error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
})
output$frame <- renderUI({
tags$iframe(src = rvestScraper[[input$error_item]]$url, height = 600, width = 1500)
})
output$items <- renderUI({
print(length(global$error_comps))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_comps,
selected = global$error_comps[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_comps)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_comps <- setdiff(global$error_comps, input$error_item)
print(length(unlist(global$error_comps)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
print("input$error_item")
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(c(input$target_text, input$get_xpath, input$error_item, global$doc), {
req(nchar(input$target_text) > 0)
req(!is.null(global$doc))
print("input$target_text")
print(input$target_text)
print("doc")
print(global$doc)
print("zzz")
is_json <- global$doc %>% jsonlite::validate()
if(is_json){
warning("document is of type json")
shinyalert(text = "document is of type json - changing to '<html>'")
global$doc <- "<html>"
txt <- "/html"
}else{
txt <- SteveAI::getXPathByText(doc = global$doc, text = input$target_text)
}
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
print("rr")
})
observeEvent(input$xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- global$items
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
output$curl <- renderPrint(global$curl_output)
})
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
output$curl <- renderPrint(global$domain_output)
})
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
}
dont_run()
SteveAI::run()
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_comps = error_comps, xpath_output = NULL)
observe({
keep <- which((error_items[, input$error_terms] %>% rowSums()) > 0)
global$error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
})
output$frame <- renderUI({
tags$iframe(src = rvestScraper[[input$error_item]]$url, height = 600, width = 1500)
})
output$items <- renderUI({
print(length(global$error_comps))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_comps,
selected = global$error_comps[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_comps)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_comps <- setdiff(global$error_comps, input$error_item)
print(length(unlist(global$error_comps)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
print("input$error_item")
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(c(input$target_text, input$get_xpath, input$error_item, global$doc), {
req(nchar(input$target_text) > 0)
req(!is.null(global$doc))
print("input$target_text")
print(input$target_text)
print("doc")
print(global$doc)
print("zzz")
is_json <- global$doc %>% jsonlite::validate()
if(is_json){
warning("document is of type json")
shinyalert(text = "document is of type json - changing to '<html>'")
global$doc <- "<html>"
txt <- "/html"
}else{
txt <- SteveAI::getXPathByText(doc = global$doc, text = input$target_text)
}
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
print("rr")
})
observeEvent(input$xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- global$items
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
output$curl <- renderPrint(global$curl_output)
})
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
output$curl <- renderPrint(global$domain_output)
})
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
dont_run <- function(){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI/"
setwd(SteveAI_dir)
load(file.path(SteveAI_dir, "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue("{SteveAI_dir}rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
bb <- log_data[which(log_data$comp_name %>% is.na), ]
log_data$n_nodes
log_data %<>%
dplyr::group_by(comp_name) %>%
dplyr::mutate(db_consist = (sum(n_today_jobs, na.rm = TRUE) + sum(n_duplicate_jobs, na.rm = TRUE) == sum(n_nodes, na.rm = TRUE)))
log_data$db_consist
error_terms <- c("wrong_xpath", "diff_length_links_id", "wrong_url", "status_404",
"timeout", "no_encod", "func_miss", "curl_error", "connect_reset",
"no_resolve_host")
ui = fluidPage({
mainPanel(
tabsetPanel(
tabPanel("Overview", fluidRow(
numericInput(
inputId = "min_nodes",
label = "Min amt nodes:",
min = 0,
value = 0,
max = 500
),
selectInput(
inputId = "logicals",
label = "Filter logicals",
choices = names(error_type_ident),
multiple = TRUE,
selectize = TRUE,
selected = NULL
),
checkboxInput(
inputId = "obj_has_txt",
label = "Missing object in code",
value = FALSE
),
DTOutput('tbl_all')
)
),
tabPanel(
title = "Analyse",
uiOutput("items"),
DTOutput('tbl_single'),
fluidRow(
column(width = 3,
checkboxGroupInput(inputId = "error_terms", label = "Error terms", choices = error_terms, selected = error_terms),
actionButton(inputId = "open_scrape_url", label = "Open current url:"),
actionButton(inputId = "google_search", label = "Search for new page at google:"),
actionButton(inputId = "remove", label = "Remove this item"),
actionButton(inputId = "curl", label = "Try curl in cmd."),
actionButton(inputId = "domain", label = "Ping domain."),
shiny::fluidRow(
textInput(inputId = "new_url", label = "New url:"),
actionButton(inputId = "add_new_url", label = "Add new url!")
),
textInput(inputId = "target_text", label = "Text for xpath:", value = "m/w"),
uiOutput("xpath"),
actionButton(inputId = "get_xpath", label = "Get xpath"),
actionButton(inputId = "use_xpath", label = "Use xpath"),
verbatimTextOutput(outputId = "curl"),
textInput(inputId = "no_job_id", label = "Text to identify valid no jobs:", value = ""),
actionButton(inputId = "add_nojob_id", label = "Add no job id"),
actionButton(inputId = "finish_item", label = "Mark as done."),
),
column(width = 9,
htmlOutput("frame")
)
)
)
)
)
})
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
keep <- which((error_items[, error_terms] %>% rowSums()) > 0)
error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_comps = error_comps, xpath_output = NULL)
observe({
keep <- which((error_items[, input$error_terms] %>% rowSums()) > 0)
global$error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
})
output$frame <- renderUI({
tags$iframe(src = rvestScraper[[input$error_item]]$url, height = 600, width = 1500)
})
output$items <- renderUI({
print(length(global$error_comps))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_comps,
selected = global$error_comps[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_comps)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_comps <- setdiff(global$error_comps, input$error_item)
print(length(unlist(global$error_comps)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
print("input$error_item")
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(c(input$target_text, input$get_xpath, input$error_item, global$doc), {
req(nchar(input$target_text) > 0)
req(!is.null(global$doc))
print("input$target_text")
print(input$target_text)
print("doc")
print(global$doc)
print("zzz")
is_json <- global$doc %>% jsonlite::validate()
if(is_json){
warning("document is of type json")
shinyalert(text = "document is of type json - changing to '<html>'")
global$doc <- "<html>"
txt <- "/html"
}else{
txt <- SteveAI::getXPathByText(doc = global$doc, text = input$target_text)
}
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
print("rr")
})
observeEvent(input$xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- global$items
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
output$curl <- renderPrint(global$curl_output)
})
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
output$curl <- renderPrint(global$domain_output)
})
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
}
dont_run()
url <- 'https://jobs.definiens.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=&optionsFacetsDD_department=&optionsFacetsDD_customfield1='
url %>% httr::GET()
library(rvest)
library(httr)
library(magrittr)
library(glue)
library(urltools)
library(SteveAI)
source("SteveAI/R/is_job_offer_page.R")
source("R/is_job_offer_page.R")
source("SteveAI/R/handle_links.R")
source("R/handle_links.R")
source("SteveAI/R/link_checker.R")
source("R/build_scraper/link_checker.R")
source('R/build_scraper/target_text.R')
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
url <- rvestScraper$DEFINIENS$domain
xx <- find_job_page(url, ses = ses)
url <- 'https://jobs.definiens.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=&optionsFacetsDD_department=&optionsFacetsDD_customfield1='
url
url <- rvestScraper$DEFINIENS$domain
rvestScraper$DEFINIENS$domain
url <- rvestScraper$DEFINIENS$domain
url
rvestScraper$DEFINIENS <- NULL
save(rvestScraper, file = "scraper_rvest.RData")
dont_run()
url <- rvestScraper$BRENNTAG$domain
url
xx <- find_job_page(url, ses = ses)
xx <- find_job_page(url, ses = ses)
alarm()
alarm()
alarm()
alarm()
beep <- function(n = 3){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
beep()
beep <- function(n = 3){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
xx <- find_job_page(url, ses = ses)
beep()
