rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
dont_run <- function(){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI/"
setwd(SteveAI_dir)
load(file.path(SteveAI_dir, "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue("{SteveAI_dir}rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
bb <- log_data[which(log_data$comp_name %>% is.na), ]
log_data$n_nodes
log_data %<>%
dplyr::group_by(comp_name) %>%
dplyr::mutate(db_consist = (sum(n_today_jobs, na.rm = TRUE) + sum(n_duplicate_jobs, na.rm = TRUE) == sum(n_nodes, na.rm = TRUE)))
log_data$db_consist
error_terms <- c("wrong_xpath", "diff_length_links_id", "wrong_url", "status_404",
"timeout", "no_encod", "func_miss", "curl_error", "connect_reset",
"no_resolve_host")
ui = fluidPage({
mainPanel(
tabsetPanel(
tabPanel("Overview", fluidRow(
numericInput(
inputId = "min_nodes",
label = "Min amt nodes:",
min = 0,
value = 0,
max = 500
),
selectInput(
inputId = "logicals",
label = "Filter logicals",
choices = names(error_type_ident),
multiple = TRUE,
selectize = TRUE,
selected = NULL
),
checkboxInput(
inputId = "obj_has_txt",
label = "Missing object in code",
value = FALSE
),
DTOutput('tbl_all')
)
),
tabPanel(
title = "Analyse",
uiOutput("items"),
DTOutput('tbl_single'),
fluidRow(
column(width = 3,
checkboxGroupInput(inputId = "error_terms", label = "Error terms", choices = error_terms, selected = error_terms),
actionButton(inputId = "open_scrape_url", label = "Open current url:"),
actionButton(inputId = "google_search", label = "Search for new page at google:"),
actionButton(inputId = "remove", label = "Remove this item"),
actionButton(inputId = "curl", label = "Try curl in cmd."),
actionButton(inputId = "domain", label = "Ping domain."),
shiny::fluidRow(
textInput(inputId = "new_url", label = "New url:"),
actionButton(inputId = "add_new_url", label = "Add new url!")
),
textInput(inputId = "target_text", label = "Text for xpath:", value = "m/w"),
uiOutput("xpath"),
actionButton(inputId = "get_xpath", label = "Get xpath"),
actionButton(inputId = "use_xpath", label = "Use xpath"),
verbatimTextOutput(outputId = "curl"),
textInput(inputId = "no_job_id", label = "Text to identify valid no jobs:", value = ""),
actionButton(inputId = "add_nojob_id", label = "Add no job id"),
actionButton(inputId = "finish_item", label = "Mark as done."),
),
column(width = 9,
htmlOutput("frame")
)
)
)
)
)
})
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
keep <- which((error_items[, error_terms] %>% rowSums()) > 0)
error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
server = function(input, output) {
global <- reactiveValues(curl_output = NULL, error_comps = error_comps, xpath_output = NULL)
observe({
keep <- which((error_items[, input$error_terms] %>% rowSums()) > 0)
global$error_comps <- error_items[keep, ] %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
})
output$frame <- renderUI({
tags$iframe(src = rvestScraper[[input$error_item]]$url, height = 600, width = 1500)
})
output$items <- renderUI({
print(length(global$error_comps))
selectInput(
inputId = "error_item",
label = "Error item:",
choices = global$error_comps,
selected = global$error_comps[1]
)
})
observeEvent(input$finish_item, {
print(input$finish_item)
print(input$error_item)
print(length(unlist(global$error_comps)))
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
global$error_comps <- setdiff(global$error_comps, input$error_item)
print(length(unlist(global$error_comps)))
})
observeEvent(input$add_nojob_id, {
rvestScraper[[input$error_item]]$no_job_id <- input$no_job_id
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$error_item, {
req(input$error_item)
print(input$error_item)
print("input$error_item")
scrape_url <- rvestScraper[[input$error_item]]$url
global$doc <- tryCatch(expr = scrape_url %>%
httr::GET() %>%
httr::content(type = "text"),
error = function(e) NULL
)
})
observeEvent(c(input$target_text, input$get_xpath, input$error_item, global$doc), {
req(nchar(input$target_text) > 0)
req(!is.null(global$doc))
print("input$target_text")
print(input$target_text)
print("doc")
print(global$doc)
print("zzz")
is_json <- global$doc %>% jsonlite::validate()
if(is_json){
warning("document is of type json")
shinyalert(text = "document is of type json - changing to '<html>'")
global$doc <- "<html>"
txt <- "/html"
}else{
txt <- SteveAI::getXPathByText(doc = global$doc, text = input$target_text)
}
output$xpath <- renderUI({
textInput(inputId = "xpath", label = "xpath:", value = txt %>% toString)
})
print("rr")
})
observeEvent(input$xpath, {
global$items <- global$doc %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = input$xpath) %>%
rvest::html_text()
print(global$items)
global$xpath_output <- global$items
output$curl <- renderPrint(global$xpath_output)
})
observeEvent(input$add_new_url, {
rvestScraper[[input$error_item]]$url <- input$new_url
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
})
observeEvent(input$curl, {
url <- rvestScraper[[input$error_item]]$url
global$curl_output <- system(command = glue::glue("curl {url}"), intern = TRUE)
output$curl <- renderPrint(global$curl_output)
})
observeEvent(input$domain, {
url <- rvestScraper[[input$error_item]]$url
domain <- urltools::domain(url)
global$domain_output <- httr::GET(url = domain)
output$curl <- renderPrint(global$domain_output)
})
observeEvent(input$remove, {
print(rvestScraper[[input$error_item]])
rvestScraper[[input$error_item]] <- NULL
save(rvestScraper, file = file.path(SteveAI_dir, "scraper_rvest.RData"))
### add: are you sure?
})
observeEvent(input$google_search, {
glue::glue("https://www.google.de/search?q={input$error_item}+jobs") %>%
browseURL()
})
observeEvent(input$open_scrape_url, {
rvestScraper[[input$error_item]]$url %>%
browseURL()
})
data <- reactive({
req(input$min_nodes)
log_data %<>% dplyr::filter(as.numeric(n_nodes) >= input$min_nodes | is.na(n_nodes))
log_data %<>% dplyr::filter(!is.na(missing_object) == input$obj_has_txt)
log_data
})
output$tbl_all = renderDT(
datatable(data(), filter = 'top'), options = list(lengthChange = FALSE)
)
output$tbl_single = renderDT(
datatable(data() %>% dplyr::filter(comp_name == input$error_item), filter = 'top'), options = list(lengthChange = FALSE)
)
}
shinyApp(ui, server)
}
dont_run()
url <- 'https://jobs.definiens.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=&optionsFacetsDD_department=&optionsFacetsDD_customfield1='
url %>% httr::GET()
library(rvest)
library(httr)
library(magrittr)
library(glue)
library(urltools)
library(SteveAI)
source("SteveAI/R/is_job_offer_page.R")
source("R/is_job_offer_page.R")
source("SteveAI/R/handle_links.R")
source("R/handle_links.R")
source("SteveAI/R/link_checker.R")
source("R/build_scraper/link_checker.R")
source('R/build_scraper/target_text.R')
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
url <- rvestScraper$DEFINIENS$domain
xx <- find_job_page(url, ses = ses)
url <- 'https://jobs.definiens.com/search/?createNewAlert=false&q=&locationsearch=&optionsFacetsDD_country=&optionsFacetsDD_department=&optionsFacetsDD_customfield1='
url
url <- rvestScraper$DEFINIENS$domain
rvestScraper$DEFINIENS$domain
url <- rvestScraper$DEFINIENS$domain
url
rvestScraper$DEFINIENS <- NULL
save(rvestScraper, file = "scraper_rvest.RData")
dont_run()
url <- rvestScraper$BRENNTAG$domain
url
xx <- find_job_page(url, ses = ses)
xx <- find_job_page(url, ses = ses)
alarm()
alarm()
alarm()
alarm()
beep <- function(n = 3){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
beep()
beep <- function(n = 3){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
xx <- find_job_page(url, ses = ses)
beep()
url <- "http://www.hermes.de"  #rvestScraper[[comp_name]]$domain
url
xx <- find_job_page(url, ses = ses)
library(rvest)
library(httr)
library(magrittr)
library(glue)
library(urltools)
library(SteveAI)
source("SteveAI/R/is_job_offer_page.R")
source("R/is_job_offer_page.R")
source("SteveAI/R/handle_links.R")
source("R/handle_links.R")
source("SteveAI/R/link_checker.R")
source("R/build_scraper/link_checker.R")
source('R/build_scraper/target_text.R')
beep <- function(n = 2){
for(i in seq(n)){
system("rundll32 user32.dll,MessageBeep -1")
Sys.sleep(.5)
}
}
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
# scrape searcher does not move forward:
#rvestScraper$APLANT$domain <- "https://www.sunbeltrentals.co.uk/"
comp_name <- "DEUTSCHE-BANK"
url <- "http://www.hermes.de"  #rvestScraper[[comp_name]]$domain
url
url %>% browseURL()
xx <- find_job_page(url, ses = ses)
beep()
url <- "http://www.hermes.de"  #rvestScraper[[comp_name]]$domain
url
xx <- find_job_page(url, ses = ses)
beep()
winner_url <- xx$parsed_links$href[xx$winner]
winner_url
paste0(winner_url, "#:~:text=", url_encode(xx$candidate_meta[[4]][1])) %>% browseURL()
xx
xx$parsed_links$href
url <- "http://www.lidl.de"  #rvestScraper[[comp_name]]$domain
url
url %>% browseURL()
xx <- find_job_page(url, ses = ses)
beep()
nonblocking <- isTRUE(getOption("curl_interrupt", TRUE))
output <- .Call(R_curl_fetch_memory, enc2utf8(url), handle,
nonblocking)
R_curl_fetch_memory
url
handle
nonblocking
output <- .Call(R_curl_fetch_memory, enc2utf8(url), handle,
nonblocking)
get_doc_phantom <- function(url, ses, pjs){
url <- as.character(url)
tryCatch(ses$go(url), error = function(e){
warning("Need to restart phantom webdriver")
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
ses$go(url)
})
url_before <- tryCatch(ses$getUrl(), error = function(e) return(""))
domain <- url_before %>%
urltools::domain()
# %>%
# gsub(pattern = "www.", replacement = "")
doc <- tryCatch(ses$findElement(xpath = "/*")$getAttribute(name = "innerHTML") %>%
xml2::read_html(), error = function(e){
return("")
})
return(
list(doc = doc, domain = domain, ses = ses, pjs = pjs, url_before = url_before)
)
}
parse_link <- function(target_link, iter_nr, link_meta, use_selenium = FALSE, use_phantom = TRUE, ses = NULL, remDr = NULL){
links <- link_meta$links
all_docs <- link_meta$all_docs
html_texts <- link_meta$html_texts
matches <- link_meta$matches
all_links <- link_meta$all_links
counts <- link_meta$counts
parsed_links <- link_meta$parsed_links
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
# workaround until frames are supported
if(is.na(link)){
exclude <- links$href %in% link
links <- links[!exclude, ]
message("skipping NA (frame link)")
return()
}
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(url = link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else if(use_phantom){
out <- get_doc_phantom(url = link, ses = ses, pjs = pjs)
ses <- out$ses
domain <- out$domain
url_before <- out$url_before
all_docs[[id]] <- out$doc
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- tryCatch(htm2txt::htm2txt(as.character(all_docs[[id]])),
error = function(e){
message(e)
warning(e)
return("")
})
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
message("No content")
warning("No content")
return(
list(
links = links[-1, ], all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
#all_docs[[id]] %>% SteveAI::showHtmlPage()
# html_texts[[id]] %>% cat
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# todo: cant find src in code - maybe have to use swith to frame but cant use it as new link then
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
# speculative change: dont replace all values
if(length(iframe_links_raw)){
iframe_links_raw[is.na(iframe_links_raw)] <- ""
}
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links[iter_nr, ]$href <- target_link$href
parsed_links[iter_nr, ]$text <- target_link$text
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links, filter_domain = FALSE)
links <- links[!duplicated(links$href), ]
links$href
links <- rbind(parsed_links, links)
head(links)
exclude <- links$href %in% parsed_links$href
head(exclude)
links <- links[!exclude, ] %>% sort_links()
# taleo careersection jobsearch
head(links$href)
# links$href <- gsub(x = links$href, pattern = "www.www.", replacement = "www.", fixed = TRUE)
# can fail
links <- check_for_button(links, url_before)
return(
list(
links = links, all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
check_for_button <- function(links, url_before, ses, pjs){
# if fail here restart ses and go to url_before
doc <- ses$findElement(xpath = "/*")
doc_len_before <- doc$getAttribute("innerHTML") %>% nchar
xp <- generate_button_xpath()
buttons <- ses$findElements(xpath = xp)
for(nr in seq(buttons)){
tryCatch(buttons[[nr]]$click(), error = function(e) message(e))
}
input_xpath <- "//input[@type = 'submit' or @value = 'Search Jobs' or contains(@value, 'Search') or @title = 'Search Jobs' or @value='Suche starten']"
inputs <- ses$findElements(xpath = input_xpath)
job_related_page_xp <- "//*[contains(text(), 'Find a job') or contains(text(), 'Search and apply') or contains(text(), 'Global Career Opportunities') or contains(text(), 'Search Jobs') or contains(text(), 'Search for jobs') or contains(text(), 'Find Jobs')  or contains(text(), 'Aktuelle Stellenangebote') or contains(text(), 'gewünschte Stelle')  or contains(text(), 'job search') or contains(text(), 'Search Current Openings')  or contains(text(), 'Careers')]"
is_job_related <- ses$findElements(xpath = job_related_page_xp) %>% length
is_job_related
# todo: finish
#
button_xpath <- "//button[./parent::*//*[contains(text(), 'GO')] or @title = 'Search for Jobs']"
buttons <- ses$findElements(xpath = button_xpath)
if(is_job_related){
for(nr in seq(buttons)){
tryCatch(buttons[[nr]]$click(), error = function(e) message(e))
}
}
if(is_job_related){
for(nr in seq(inputs)){
message("Found and trying a relevant input")
tryCatch(inputs[[nr]]$click(), error = function(e) message(e))
}
}
alinks <- ses$findElements(xpath = "//a[@id = 'taleoSearchSubmit']")
if(is_job_related){
for(nr in seq(alinks)){
message("Found and trying a relevant input")
tryCatch(alinks[[nr]]$click(), error = function(e) message(e))
}
}
url_after <- ses$getUrl()
doc <- ses$findElement(xpath = "/*")
doc_len_after <- doc$getAttribute("innerHTML") %>% nchar
button_relevant <- url_after != url_before | doc_len_before != doc_len_after
if(button_relevant){
message("Found a relevant button")
links <- rbind(
data.frame(
href = url_after,
text = "Caused by SteveAI button click"
),
links
)
}
return(links)
}
