#   append = TRUE,
#   row.names = FALSE,
#   col.names = FALSE
# )
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
url = scraper$url
if(!is.null(out)){
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = url,
logger_name = logger_name,
date_today = date_today
)
}else{
warning(glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."))
scrape_log_warn(
target_name = name,
url = url,
msg = glue::glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."),
logger_name = logger_name
)
}
}
scraper$href
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
setwd("~")
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 4
responses <- list.files(folder_name)
nms <- responses %>%
sapply(FUN = function(response){
strsplit(x = response, split = "_") %>% unlist %>% .[1]
})
if(!length(nms)) stop("Did not find any downloaded files.")
file.copy(from = "~/rvest_scraper.db", to = glue::glue("~/rvest_scraper_{date_today}_BACKUP.db"))
nr <- 1
setwd("~")
log_path <- "~"
UC_Name = "rvest_single"
logger_name = "sivis"
file_name <- file.path(log_path, paste0(UC_Name, "_", Sys.Date(), ".log"))
initialize(logger_name = logger_name, trennzeichen = "___", log_path = "~", UC_Name = "rvest_single", file_name)
setwd("~")
library(magrittr)
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
fetch_jobid %>% head
fetch_job_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time)
)
fetch_job_time %>% head
log_path <- "~"
UC_Name = "rvest_single"
logger_name = "sivis"
file_name <- file.path(log_path, paste0(UC_Name, "_", Sys.Date(), ".log"))
initialize(logger_name = logger_name, trennzeichen = "___", log_path = "~", UC_Name = "rvest_single", file_name)
flog.info("test", name = logger_name)
log_Data <- file_name %>%
readLines
log_Data
date_today = Sys.Date()
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 4
responses <- list.files(folder_name)
nms <- responses %>%
sapply(FUN = function(response){
strsplit(x = response, split = "_") %>% unlist %>% .[1]
})
if(!length(nms)) stop("Did not find any downloaded files.")
file.copy(from = "~/rvest_scraper.db", to = glue::glue("~/rvest_scraper_{date_today}_BACKUP.db"))
nr <- 1
setwd("~")
rvestScraper
SteveAI::rvestScraper
SteveAI::rvestScraper[[nr]]
SteveAI::rvestScraper[[nr]]
scraper$href
scraper[[1]]
scraper[[1]]
SteveAI::rvestScraper
nr
scraper <- SteveAI::rvestScraper[[nr]]
SteveAI::run()
library(SteveAI)
SteveAI::run()
library(stringr)
library(shiny)
library(DT)
library(magrittr)
library(glue)
library(xml2)
library(rvest)
get_error_items <- function(){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI"
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue("~/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
error_items
}
comp_names <- get_error_items() %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI"
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- file.path(SteveAI_dir, "/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI"
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- file.path(SteveAI_dir, "/rvest_single_{date_today}.log")
log_file
log_file <- file.path(SteveAI_dir, "log/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_file <- glue(SteveAI_dir, "/log/rvest_single_{date_today}.log")
log_file
log_data <- parse_logs(log_file = log_file)
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_file
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_file
getwd()
SteveAI::run()
SteveAI::run()
SteveAI::run()
SteveAI::run()
SteveAI::run()
getwd()
SteveAI::run()
library(stringr)
library(shiny)
library(DT)
library(magrittr)
library(glue)
library(xml2)
library(rvest)
get_error_items <- function(){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI"
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
error_items
}
comp_names <- get_error_items() %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
get_error_items <- function(SteveAI_dir){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
error_items
}
SteveAI_dir <- "C:/Users/User11/Documents/"
comp_names <- get_error_items() %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
SteveAI_dir <- "C:/Users/User11/Documents/"
comp_names <- get_error_items() %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
get_error_items <- function(SteveAI_dir){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
error_items
}
SteveAI_dir <- "C:/Users/User11/Documents/"
comp_names <- get_error_items(SteveAI_dir) %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
get_error_items <- function(SteveAI_dir){
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue(SteveAI_dir, "/logs/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
error_items
}
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI/"
comp_names <- get_error_items(SteveAI_dir) %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
log_results2 <- list()
load(file.path("~", "scraper_rvest.RData"))
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
comp_name <- comp_names[2]
for(comp_name in comp_names){
print(comp_name)
url <- rvestScraper[[comp_name]]$domain
log_results2[[url]] <- tryCatch(
find_job_page(url, ses, use_phantom = TRUE),
error = function(e){
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
print("Call to find_job_page failed with:")
print(e)
return("")
}
)
save(log_results2, file = "data/log_results2.RData")
}
comp_name
load("data/log_results2.RData")
comp_name <- comp_names[1]
url <- rvestScraper[[comp_name]]$domain
log_results2[[url]]$counts
url
rvestScraper[[comp_name]]
url <- rvestScraper[[comp_name]]$url
log_results2[[url]]$counts
log_results2[[url]]
url
log_results2[[1]]$counts
log_results2[[1]]
log_results2[1]
log_results2[2]
names(log_results2)
url <- rvestScraper[[comp_name]]$url
url <- names(log_results2)[nr]
log_results2[[url]]$counts
load("data/log_results2.RData")
comp_name <- comp_names[1]
url <- names(log_results2)[nr]
log_results2[[url]]$counts
log_results2[[url]]
url
nr <- 2
url <- names(log_results2)[nr]
log_results2[[url]]$counts
winner <- log_results2[[url]]$winner
out <- log_results2[[url]]
log_results2[[url]]$parsed_links$href[winner]
log_results2[[url]]$parsed_links$href[winner] %>% browseURL()
library(stringr)
library(shiny)
library(DT)
library(magrittr)
library(glue)
library(xml2)
library(rvest)
log_results2[[url]]$parsed_links$href[winner]
log_results2[[url]]$parsed_links$href[winner] %>% browseURL()
rr <- extract_target_text(parsing_results = out)
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
rr <- extract_target_text(parsing_results = out)
rr$candidate_meta
rr$candidate_meta
rr
doc <- parsing_results$doc %>% xml2::read_html()
doc <- parsing_results$doc %>% xml2::read_html()
# doc %>% SteveAI::showHtmlPage()
# parsing_results$all_docs[[parsing_results$winner]] %>% SteveAI::showHtmlPage()
target_text <- get_target_text(parsing_results)
# target_text <- "referent"
target_text
#doc %>% SteveAI::showHtmlPage()
xpath <- SteveAI::getXPathByText(text = target_text, doc = doc, add_class = TRUE, exact = TRUE)
xpath
# target_text <- "Account Executive"
target_text
target_text <- "Account Executive"
#doc %>% SteveAI::showHtmlPage()
xpath <- SteveAI::getXPathByText(text = target_text, doc = doc, add_class = TRUE, exact = TRUE)
xpath
load("data/log_results2.RData")
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
comp_name <- comp_names[1]
nr <- 2
url <- names(log_results2)[nr]
log_results2[[url]]$counts
winner <- log_results2[[url]]$winner
out <- log_results2[[url]]
log_results2[[url]]$parsed_links$href[winner]
log_results2[[url]]$parsed_links$href[winner] %>% browseURL()
library(stringr)
library(shiny)
library(DT)
library(magrittr)
library(glue)
library(xml2)
library(rvest)
log_results2[[url]]$counts
winner <- log_results2[[url]]$winner
out <- log_results2[[url]]
log_results2[[url]]$parsed_links$href[winner]
rr <- extract_target_text(parsing_results = out)
rr$candidate_meta
doc <- parsing_results$doc %>% xml2::read_html()
# doc %>% SteveAI::showHtmlPage()
# parsing_results$all_docs[[parsing_results$winner]] %>% SteveAI::showHtmlPage()
target_text <- get_target_text(parsing_results)
# target_text <- "Account Executive"
target_text
target_text <- "Account Executive"
doc %>% SteveAI::showHtmlPage()
parsing_results$doc
parsing_results$doc %>% showHtmlPage()
SteveAI::run()
url <- "https://karriere.nrw/stellenausschreibung/dba41541-8ed9-4449-8f79-da3cda0cc07c
"
url %>% xml2::read_html()
url <- "https://karriere.nrw/stellenausschreibung/dba41541-8ed9-4449-8f79-da3cda0cc07c"
url %>% xml2::read_html()
doc <- url %>% xml2::read_html()
doc %>% showHtmlPage()
url <- "https://karriere.nrw/stellenausschreibung/dba41541-8ed9-4449-8f79-da3cda0cc07c"
pjs <- webdriver::run_phantomjs()
ses <- webdriver::Session$new(port = pjs$port)
ses$go(url)
ses$takeScreenshot()
ses$takeScreenshot()
url <- "https://www.google.de"
ses$go(url)
ses$takeScreenshot()
url <- "https://karriere.nrw/stellenausschreibung/dba41541-8ed9-4449-8f79-da3cda0cc07c"
pjs <- webdriver::run_phantomjs()
ses <- webdriver::Session$new(port = pjs$port)
ses$go(url)
ses$takeScreenshot()
elem <- ses$findElement(xpath = "/*")
elem$getAttribute(name = "innerHTML")
doc <- url %>% xml2::read_html()
doc
doc %>% showHtmlPage()
url <- "https://karriere.nrw/suche"
pjs <- webdriver::run_phantomjs()
ses <- webdriver::Session$new(port = pjs$port)
ses$go(url)
ses$takeScreenshot()
elem <- ses$findElement(xpath = "/*")
elem$getAttribute(name = "innerHTML")
doc <- url %>% xml2::read_html()
doc %>% showHtmlPage()
url <- "https://karriere.nrw"
pjs <- webdriver::run_phantomjs()
ses <- webdriver::Session$new(port = pjs$port)
ses$go(url)
ses$takeScreenshot()
elem <- ses$findElement(xpath = "/*")
elem$getAttribute(name = "innerHTML")
doc <- url %>% xml2::read_html()
doc %>% showHtmlPage()
library(RSelenium)
port_nr = 4452
remDr <- RSelenium::remoteDriver(
remoteServerAddr = "localhost",
port = port_nr
)
# port_nr = 4449
# sudo: no tty present and no askpass program specified
# --> https://stackoverflow.com/a/39553081/3502164
tryCatch(
system(
paste0("docker run -d -p ", port_nr,":4444 selenium/standalone-firefox:2.53.0")
), error = function(e) warning("system docker call failed")
)
# port_nr = 4449
# sudo: no tty present and no askpass program specified
# --> https://stackoverflow.com/a/39553081/3502164
tryCatch(
system(
paste0("docker run -d -p ", port_nr,":4444 selenium/standalone-firefox:2.53.0")
), error = function(e) warning("system docker call failed")
)
# port_nr = 4449
# sudo: no tty present and no askpass program specified
# --> https://stackoverflow.com/a/39553081/3502164
tryCatch(
system(
paste0("docker run -d -p ", port_nr,":4444 selenium/standalone-firefox:2.53.0")
), error = function(e) warning("system docker call failed")
)
remDr$open()
# So that all elements can be seen and are not hidden
remDr$setWindowSize(width = 4800, height = 2400)
remDr$navigate(url)
remDr$screenshot(display = TRUE)
url <- "https://karriere.nrw/stellenausschreibung/dba41541-8ed9-4449-8f79-da3cda0cc07c"
remDr$navigate(url)
remDr$screenshot(display = TRUE)
url <- "https://karriere.nrw/"
remDr$navigate(url)
remDr$screenshot(display = TRUE)
url <- "https://karriere.nrw/suche"
remDr$navigate(url)
remDr$screenshot(display = TRUE)
load("data/log_results2.RData")
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
comp_name <- comp_names[1]
nr <- 2
url <- names(log_results2)[nr]
log_results2[[url]]$counts
winner <- log_results2[[url]]$winner
parsing_results <- log_results2[[url]]
parsing_results$winner
parsing_results$all_docs[[1]]
parsing_results$all_docs[[1]] == parsing_results$doc
log_results2[[url]]$parsed_links$href[winner]
parsing_results$all_docs[[parsing_results$winner]]
parsing_results$all_docs[[parsing_results$winner]] %>% showHtmlPage()
log_results2[[url]]$parsed_links$href[winner]
SteveAI::run()
SteveAI::run()
SteveAI::run()
SteveAI::run()
SteveAI::run()
SteveAI::run()
SteveAI::run()
