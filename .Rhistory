print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper, name = name, date_today = date_today),
error = function(e){
print(e)
name <- names(SteveAI::rvestScraper)[nr]
url <- SteveAI::rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
if(is.null(data_raw[[name]]) | length(data_raw[[name]]) < 2) next
# data_raw
# has_Id <- FALSE #names(data_raw[[name]])
# if(has_Id){
#
# }else{
#
#   # build id - take all non-empty columns that do not have blacklist items
#   apply(data_raw, 2, nchar)
#
# }
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
# fileNameJobDesc = paste0("dataRvest/JobDescLinks_", name, "_", date_today, ".csv")
# write.csv2(x = data_raw[[name]], file = fileNameJobDesc, row.names = FALSE)
# write.table(
#   data.frame(
#     name = name,
#     duration = scrapeDuration
#   ),
#   file = durationFileName,
#   append = TRUE,
#   row.names = FALSE,
#   col.names = FALSE
# )
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
url = scraper$url
if(!is.null(out)){
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = url,
logger_name = logger_name,
date_today = date_today
)
}else{
warning(glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."))
scrape_log_warn(
target_name = name,
url = url,
msg = glue::glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."),
logger_name = logger_name
)
}
}
get_error_items <- function(){
library(stringr)
library(shiny)
library(DT)
library(magrittr)
library(glue)
library(xml2)
library(rvest)
date_today <- Sys.Date()
browser_path <- "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
browser_path %>% file.exists()
options(browser = browser_path)
#source("C:/Users/User11/Desktop/Transfer/TMP/mypkg2/R/sivis.R")
SteveAI_dir <- "C:/Users/User11/Documents/SteveAI"
setwd(SteveAI_dir)
load(file.path("~", "scraper_rvest.RData"))
source(file.path(SteveAI_dir, "R/log_analysis_func.R"))
log_file <- glue("~/rvest_single_{date_today}.log")
log_data <- parse_logs(log_file = log_file)
log_data %>% head
error_items <- log_data %>%
dplyr::filter(level == "ERROR")
error_items
}
comp_names <- get_error_items() %>%
dplyr::filter(n_nodes == 0 | curl_error == TRUE) %>%
dplyr::select(comp_name) %>%
unlist %>%
unname
log_results2 <- list()
comp_name <- comp_names[2]
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
comp_name <- comp_names[2]
for(comp_name in comp_names){
print(comp_name)
url <- rvestScraper[[comp_name]]$domain
log_results2[[url]] <- tryCatch(
find_job_page(url, ses, use_phantom = TRUE),
error = function(e){
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
print("Call to find_job_page failed with:")
print(e)
return("")
}
)
save(log_results2, file = "data/log_results2.RData")
}
load(file.path("~", "scraper_rvest.RData"))
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
comp_name <- comp_names[2]
for(comp_name in comp_names){
print(comp_name)
url <- rvestScraper[[comp_name]]$domain
log_results2[[url]] <- tryCatch(
find_job_page(url, ses, use_phantom = TRUE),
error = function(e){
pjs <<- webdriver::run_phantomjs()
ses <<- webdriver::Session$new(port = pjs$port)
print("Call to find_job_page failed with:")
print(e)
return("")
}
)
save(log_results2, file = "data/log_results2.RData")
}
save(log_results2, file = "data/log_results2.RData")
log_results2[[url]]
comp_names
url <- comp_names[1]
log_results2[[url]]
comp_names
url <- comp_names[1]
url
log_results2
url <- comp_names[1]
url
comp_name <- comp_names[1]
url <- rvestScraper[[comp_name]]$domain
setwd("~")
library(magrittr)
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
fetch_jobid %>% head
setwd("~")
library(magrittr)
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
fetch_jobid %>% head
fetch_job_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time)
)
fetch_job_time %>% head
SteveAI::run()
library(SteveAI)
SteveAI::run()
SteveAI::run()
SteveAI::run()
library(SteveAI)
library(SteveAI)
SteveAI::run()
responses <- list.files(folder_name)
nms <- responses %>%
sapply(FUN = function(response){
strsplit(x = response, split = "_") %>% unlist %>% .[1]
})
if(!length(nms)) stop("Did not find any downloaded files.")
file.copy(from = "~/rvest_scraper.db", to = glue::glue("~/rvest_scraper_{date_today}_BACKUP.db"))
nr <- 1
setwd("~")
print(nr)
scraper <- SteveAI::rvestScraper[[nr]]
scraper
name <- names(SteveAI::rvestScraper)[nr]
file_Name <- which(name == nms) %>% names
if(!length(file_Name)){
warning("no such file with response.")
next
}
# loading variable: response here
load(file.path(folder_name, file_Name))
start <- Sys.time()
print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper, name = name, date_today = date_today),
error = function(e){
print(e)
name <- names(SteveAI::rvestScraper)[nr]
url <- SteveAI::rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
library(tryCatchLog)
library(rvest)
library(glue)
library(futile.logger)
library(magrittr)
library(DBI)
#source("SteveAI/R/logging.R")
source("R/logging.R")
print(nr)
scraper <- SteveAI::rvestScraper[[nr]]
name <- names(SteveAI::rvestScraper)[nr]
file_Name <- which(name == nms) %>% names
if(!length(file_Name)){
warning("no such file with response.")
next
}
# loading variable: response here
load(file.path(folder_name, file_Name))
start <- Sys.time()
print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper, name = name, date_today = date_today),
error = function(e){
print(e)
name <- names(SteveAI::rvestScraper)[nr]
url <- SteveAI::rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
rvestScraping <- function(response, name, scraper, date_today){
# check direct is its xml?
if(is.character(response)){
stop("The provided response is not of type xml document, but of type character. The download might have failed, check the downloaded doument for validity.")
}
status <- response %>% httr::status_code()
url = scraper$url
log_status(
status = status,
name = name,
url = url
)
encoding <- response$headers$`content-type` %>%
ifelse(is.null(.), yes = "", no = .) %>%
strsplit(split = "charset=") %>%
unlist %>%
.[2]
if(is.na(encoding)){
scrape_log_info(
target_name = name,
url = url,
msg = glue::glue("No encoding found, defaulting to UTF-8"),
logger_name = logger_name
)
}
content <- response %>%
httr::content(type = "text", encoding = encoding)
content_len <- content %>%
nchar
nodes <- content %>%
xml2::read_html() %>%
rvest::html_nodes(xpath = scraper$jobNameXpath)
log_node_len(nodes, name = name, scraper = scraper, content = content)
rvestOut <- gsub(
pattern = "\n|   |\tNew|\t",
replacement = "",
x = nodes %>% rvest::html_text()
)
if(is.na(scraper$href)){
links <- NA
}else{
links <- content %>%
xml2::read_html() %>%
html_nodes(xpath = scraper$href) %>%
html_attr(name = "href")
}
# rvestLink <- getRvestHref(
#   url = url,
#   XPath = XPath
# )
if(length(rvestOut)){
if(length(rvestOut) != length(links)){
e <- glue("Lengths of jobNames and links differ. Jobnames: '{paste0(head(rvestOut, 3), collapse = '\n')}' and links: '{paste0(head(links, 3), collapse = '\n')}'")
print("DIFFERENT LENGTHS!")
scrape_log_error(
target_name = name,
msg = e,
url = scraper$url,
logger_name = logger_name
)
links <- NA
}
out <- data.frame(
jobName = rvestOut,
links = links,
comp = name,
date = date_today,
location = "",
eingestelltAm = "",
bereich = ""
)
}else{
out <- NULL
}
return(out)
}
scrape_log_info <- function(target_name, url, msg, logger_name){
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following log is written: {msg}")
flog.info(msg = msg, name = logger_name)
}
scrape_log_warn <- function(target_name, url, msg, logger_name){
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following log is written: {msg}")
flog.warn(msg = msg, name = logger_name)
}
scrape_log_error <- function(target_name, url, msg, logger_name){
if(is.null(url)){
msg <- glue::glue("Missing url for comp_name:'{target_name}' Provided url is NULL - please provide a valid url.")
}else{
msg <- glue::glue("For comp_name:'{target_name}' with url:'{url}' the following error was recorded: {msg}") %>%
toString
}
flog.error(msg = msg, name = logger_name)
}
date_today = Sys.Date()
source('~/SteveAI/R/scheduled_Scraping_rvest.R', echo=TRUE)
#names(SteveAI::rvestScraper)
for(nr in seq(SteveAI::rvestScraper)){
print(nr)
scraper <- SteveAI::rvestScraper[[nr]]
name <- names(SteveAI::rvestScraper)[nr]
file_Name <- which(name == nms) %>% names
if(!length(file_Name)){
warning("no such file with response.")
next
}
# loading variable: response here
load(file.path(folder_name, file_Name))
start <- Sys.time()
print(name)
data_raw[[name]] <- tryCatchLog(
expr = rvestScraping(response = response, scraper = scraper, name = name, date_today = date_today),
error = function(e){
print(e)
name <- names(SteveAI::rvestScraper)[nr]
url <- SteveAI::rvestScraper[[nr]]$url
msg <- glue::glue("Scrape for comp_name:'{name}' failed for url:'{url}'. The error reads: {e}.")
scrape_log_error(
target_name = name,
msg = msg,
url = scraper$url,
logger_name = logger_name
)
}
)
if(is.null(data_raw[[name]]) | length(data_raw[[name]]) < 2) next
# data_raw
# has_Id <- FALSE #names(data_raw[[name]])
# if(has_Id){
#
# }else{
#
#   # build id - take all non-empty columns that do not have blacklist items
#   apply(data_raw, 2, nchar)
#
# }
end <- Sys.time()
scrapeDuration <- as.numeric(end - start)
# fileNameJobDesc = paste0("dataRvest/JobDescLinks_", name, "_", date_today, ".csv")
# write.csv2(x = data_raw[[name]], file = fileNameJobDesc, row.names = FALSE)
# write.table(
#   data.frame(
#     name = name,
#     duration = scrapeDuration
#   ),
#   file = durationFileName,
#   append = TRUE,
#   row.names = FALSE,
#   col.names = FALSE
# )
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
out = data_raw[[name]]
url = scraper$url
if(!is.null(out)){
write_To_DB(
db_name = db_name,
target_table_job = target_table_job,
target_table_time = target_table_time,
out = out,
target_name = name,
url = url,
logger_name = logger_name,
date_today = date_today
)
}else{
warning(glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."))
scrape_log_warn(
target_name = name,
url = url,
msg = glue::glue("No new data to insert for company: {name}. Might be due to wrong xpath, check the logs from the requests."),
logger_name = logger_name
)
}
}
scraper$href
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
setwd("~")
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 4
responses <- list.files(folder_name)
nms <- responses %>%
sapply(FUN = function(response){
strsplit(x = response, split = "_") %>% unlist %>% .[1]
})
if(!length(nms)) stop("Did not find any downloaded files.")
file.copy(from = "~/rvest_scraper.db", to = glue::glue("~/rvest_scraper_{date_today}_BACKUP.db"))
nr <- 1
setwd("~")
log_path <- "~"
UC_Name = "rvest_single"
logger_name = "sivis"
file_name <- file.path(log_path, paste0(UC_Name, "_", Sys.Date(), ".log"))
initialize(logger_name = logger_name, trennzeichen = "___", log_path = "~", UC_Name = "rvest_single", file_name)
setwd("~")
library(magrittr)
db_name <- "rvest_scraper.db"
target_table_job <- "RVEST_SINGLE_JOBS"
target_table_time <- "RVEST_SINGLE_TIME"
conn <- DBI::dbConnect(RSQLite::SQLite(), db_name)
fetch_jobid <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_job)
)
fetch_jobid %>% head
fetch_job_time <- DBI::dbGetQuery(
conn = conn,
statement = paste0("SELECT * FROM ", target_table_time)
)
fetch_job_time %>% head
log_path <- "~"
UC_Name = "rvest_single"
logger_name = "sivis"
file_name <- file.path(log_path, paste0(UC_Name, "_", Sys.Date(), ".log"))
initialize(logger_name = logger_name, trennzeichen = "___", log_path = "~", UC_Name = "rvest_single", file_name)
flog.info("test", name = logger_name)
log_Data <- file_name %>%
readLines
log_Data
date_today = Sys.Date()
print(Sys.time())
data_raw <- list()
#SteveAI_dir <- "~"
durationFileName <- glue("{SteveAI_dir}/scrapeDuration_{date_today}.csv")
folder_name <- glue("response_raw/{date_today}")
dir.create("response_raw")
dir.create(folder_name)
get_nr <- 4
responses <- list.files(folder_name)
nms <- responses %>%
sapply(FUN = function(response){
strsplit(x = response, split = "_") %>% unlist %>% .[1]
})
if(!length(nms)) stop("Did not find any downloaded files.")
file.copy(from = "~/rvest_scraper.db", to = glue::glue("~/rvest_scraper_{date_today}_BACKUP.db"))
nr <- 1
setwd("~")
rvestScraper
SteveAI::rvestScraper
SteveAI::rvestScraper[[nr]]
SteveAI::rvestScraper[[nr]]
scraper$href
scraper[[1]]
scraper[[1]]
SteveAI::rvestScraper
nr
scraper <- SteveAI::rvestScraper[[nr]]
