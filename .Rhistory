# catch document and all links
all_links <- list()
all_docs <- list()
matches <- list()
links <- sort_links(links)
links %>% head
#links[1] %>% browseURL()
iter_nr <- 0
max_iter <- 10
counts <- rep(NA, max_iter)
iter_nr <- iter_nr + 1
target_link <- links[1, ]
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- htm2txt::htm2txt(as.character(all_docs[[id]]))
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
warning("No content")
}
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# "https://careers.danone.com/de-global/" %in% links$href
# "https://careers.danone.com/de-global/" %in% links2$href
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links <- rbind(parsed_links, target_link)
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links <- rbind(parsed_links, links)
target_link
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links <- rbind(parsed_links, links)
links
exclude <- duplicated(links$href) | duplicated(links$href, fromLast = TRUE)
exclude
links <- links[!exclude, ] %>% sort_links()
links
target_link
target_link
target_link
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links) %>%
unique
links
target_link
target_link$href
target_link$href %>% nchar
target_link$href
target_link$href %>% {substring(., nchar(.))}
last_char <- link %>% {substring(., nchar(.))}
last_char <- link %>% {substring(., nchar(.))}
last_char
last_char <- substring(link, first = nchar(link))} == "/"
rm_last_char <- substring(link, first = nchar(link))} == "/"
rm_last_char <- substring(link, first = nchar(link)) == "/"
rm_last_char
ifelse(last_char, yes = substring(link, first = 1, last = nchar(link) - 1))
link <- target_link$href
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(last_char, yes = substring(link, first = 1, last = nchar(link) - 1))
ifelse(last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
filter_links <- function(links, domain, parsed_links, filter_domain = FALSE){
# some links are reported like: "//www.saturn.de/de/category/_teegeräte-476120.html".
needs_start <- substring(text = links$href, first = 1, last = 1) == "/" &
!grepl(x = links$href, pattern = "http") &
!grepl(x = links$href, pattern = "www.", fixed = TRUE)
links$href %<>%
{ifelse(test = needs_start, yes = paste0("https://www.", domain, .), no = .)}
domains <- links$href %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
# e.g. jobs.lidl.de should be same_domain as lidl.de - therefore use grepl instead of "=="
same_domain <- is.na(domains) | grepl(pattern = domain, x = domains)
hash_link <- substr(links$href, 1, 1) == "#"
is_empty <- !nchar(links$href)
is_na <- is.na(links$href)
alr_exist <- links$href %in% parsed_links
is_html <- sapply(c("mailto:", "javascript:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = tolower(links$href)) %>%
rowSums %>%
magrittr::not()
#!is_na & --> need NA for click with selenium
# links2 <- links
# links <- links2
keep <- !hash_link & !is_empty & !alr_exist & is_html
keep[is.na(keep)] <- TRUE
links %<>%
.[keep, ] %>%
{.[!duplicated(.), ]}
if(filter_domain) links %<>% .[same_domain, ]
# links %>% grepl(pattern = "jobs") %>% which %>% {links2[.]}
not_matched <- grepl(substring(text = links$href, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links$href, pattern = "https://") &
!grepl(links$href, pattern = "www.")
links$href %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
link <- target_link$href
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
links
links$href <- sapply(links$href, equalize_links)
equalize_links <- function(link){
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
}
links$href <- sapply(links$href, equalize_links)
links$href
equalize_links <- function(link){
rm_last_char <- substring(link, first = nchar(link)) == "/"
ifelse(rm_last_char, yes = substring(link, first = 1, last = nchar(link) - 1), no = link)
}
filter_links <- function(links, domain, parsed_links, filter_domain = FALSE){
links$href <- sapply(links$href, equalize_links)
# some links are reported like: "//www.saturn.de/de/category/_teegeräte-476120.html".
needs_start <- substring(text = links$href, first = 1, last = 1) == "/" &
!grepl(x = links$href, pattern = "http") &
!grepl(x = links$href, pattern = "www.", fixed = TRUE)
links$href %<>%
{ifelse(test = needs_start, yes = paste0("https://www.", domain, .), no = .)}
domains <- links$href %>%
urltools::domain() %>%
gsub(pattern = "www.", replacement = "")
# e.g. jobs.lidl.de should be same_domain as lidl.de - therefore use grepl instead of "=="
same_domain <- is.na(domains) | grepl(pattern = domain, x = domains)
hash_link <- substr(links$href, 1, 1) == "#"
is_empty <- !nchar(links$href)
is_na <- is.na(links$href)
alr_exist <- links$href %in% parsed_links
is_html <- sapply(c("mailto:", "javascript:", ".tif", ".mp4", ".mp3", ".tiff", ".png", ".gif", ".jpeg",".jpg", ".zip", ".pdf"),
FUN = grepl, x = tolower(links$href)) %>%
rowSums %>%
magrittr::not()
#!is_na & --> need NA for click with selenium
# links2 <- links
# links <- links2
keep <- !hash_link & !is_empty & !alr_exist & is_html
keep[is.na(keep)] <- TRUE
links %<>%
.[keep, ] %>%
{.[!duplicated(.), ]}
if(filter_domain) links %<>% .[same_domain, ]
# links %>% grepl(pattern = "jobs") %>% which %>% {links2[.]}
not_matched <- grepl(substring(text = links$href, first = 1, last = 1), pattern = "[a-z]", perl = TRUE) &
!grepl(links$href, pattern = "https://") &
!grepl(links$href, pattern = "www.")
links$href %<>%
{ifelse(
test = not_matched,
yes = paste0("https://www.", domain, "/", .),
no = .)
}
return(links)
}
nr
nr <- 2
url <- urls[nr]
url
nr <- 3
url <- urls[nr]
url
url <- urls[nr]
indeed_results[[url]] <- tryCatch(
find_job_page(url, remDr, TRUE),
error = function(e){
remDr <- tryCatch(start_selenium (port = 4455 + nr), error = function(e) return(""))
return("")
}
)
remDr <- start_selenium(port = 4457)
names(indeed_results) %>% .[length(.)] %>%
magrittr::equals(urls) %>% which
nr <- 3
url <- urls[nr]
indeed_results[[url]] <- tryCatch(
find_job_page(url, remDr, TRUE),
error = function(e){
remDr <- tryCatch(start_selenium (port = 4455 + nr), error = function(e) return(""))
return("")
}
)
Q
library(rvest)
library(httr)
library(magrittr)
library(urltools)
# system(
#   paste0("sudo -kS docker rm -vf $(docker ps -a -q)"),
#   input = "vistarundle1!!!"
# )
# system(
#   paste0("sudo -S docker rmi -f $(docker images -a -q)"),
#   input = "vistarundle1!!!"
# )
# powershell
# docker ps -aq | foreach {docker rm $_}
source("R/is_job_offer_page.R")
source("R/handle_links.R")
source("R/link_checker.R")
load("data/comp_urls_indeed.RData")
load("data/job_page_candidates_indeed.RData")
urls <- unlist(comp_urls)
ses <<- start_phantom()
#remDr <- start_selenium(port_nr = 4459)
#url <- "https://www.avitea.de/" --> selenium dies
#url <- "https://www.daimler.de/"
#url <- "https://www.capita.com/"
url <- "https://www.amazon.com/"
remDr = NULL
ses <<- start_phantom()
use_selenium = FALSE
use_phantom = TRUE
find_job_page <- function(url, remDr = NULL, ses = NULL, use_selenium = FALSE, use_phantom = TRUE){
iter_nr <- 0
max_iter <- 12
parsed_links <- data.frame(href = character(max_iter), text = character(max_iter))
links_per_level <- list()
docs_per_level <- list()
#links[1] %>% browseURL()
link <- url
link_meta <- create_link_meta(use_selenium, url, remDr, use_phantom, ses, link, parsed_links, max_iter)
link_meta$links %>% head
while(iter_nr < max_iter){
iter_nr <- iter_nr + 1
target_link <- link_meta$links[1, ]
if(is.na(target_link$href)){
link_meta$links <- link_meta$links[-1, ]
next
}
link_meta <- parse_link(
target_link = target_link,
iter_nr = iter_nr, link_meta = link_meta,
use_selenium = FALSE, use_phantom = TRUE,
remDr = NULL, ses = ses
)
link_meta$links %>% head
grepl(link_meta$links$href, pattern = "career.centogene.com")
}
winner <- which(max(link_meta$counts, na.rm = TRUE) == link_meta$counts)[1]
# targets <- parsed_links[max(counts, na.rm = TRUE) == counts]
# targets[1] %>% browseURL()
link_meta$counts
link_meta$parsed_links$href
link_meta$parsed_links$href[winner]
link_meta$parsed_links$href[winner] %>% browseURL()
link_meta$all_docs[[winner]] %>% SteveAI::showHtmlPage()
link_meta$all_docs[[winner]] %>% toString %>% grepl(pattern = "Elektro")
return(
list(
doc = as.character(link_meta$all_docs[[winner]]),
all_docs = lapply(link_meta$all_docs, as.character),
counts = link_meta$counts,
parsed_links = link_meta$parsed_links,
matches = link_meta$matches,
winner = winner
)
)
}
iter_nr <- 0
max_iter <- 12
parsed_links <- data.frame(href = character(max_iter), text = character(max_iter))
links_per_level <- list()
docs_per_level <- list()
#links[1] %>% browseURL()
link <- url
link_meta <- create_link_meta(use_selenium, url, remDr, use_phantom, ses, link, parsed_links, max_iter)
link_meta$links %>% head
parse_link <- function(target_link, iter_nr, link_meta, use_selenium = FALSE, use_phantom = TRUE, ses = NULL, remDr = NULL){
links <- link_meta$links
all_docs <- link_meta$all_docs
html_texts <- link_meta$html_texts
matches <- link_meta$matches
all_links <- link_meta$all_links
counts <- link_meta$counts
parsed_links <- link_meta$parsed_links
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
# workaround until frames are supported
if(is.na(link)){
exclude <- links$href %in% link
links <- links[!exclude, ]
message("skipping NA (frame link)")
return()
}
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(url = link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else if(use_phantom){
out <- get_doc_phantom(url = link, ses = ses)
ses <- out$ses
domain <- out$domain
all_docs[[id]] <- out$doc
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- tryCatch(htm2txt::htm2txt(as.character(all_docs[[id]])),
error = function(e){
message(e)
warning(e)
return("")
})
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
message("No content")
warning("No content")
return(
list(
links = links[-1, ], all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
#all_docs[[id]] %>% SteveAI::showHtmlPage()
# html_texts[[id]] %>% cat
# doc <- "https://www.bofrost.de/karriere/job/" %>% read_html
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# "https://careers.danone.com/de-global/" %in% links$href
# "https://careers.danone.com/de-global/" %in% links2$href
# todo: cant find src in code - maybe have to use swith to frame but cant use it as new link then
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
if(length(iframe_links_raw)){
if(is.na(iframe_links_raw)){
iframe_links_raw <- character()
}
}
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links[iter_nr, ]$href <- target_link$href
parsed_links[iter_nr, ]$text <- target_link$text
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links)
links <- links[!duplicated(links$href), ]
links$href
links <- rbind(parsed_links, links)
head(links)
exclude <- links$href %in% parsed_links$href
head(exclude)
links <- links[!exclude, ] %>% sort_links()
# taleo careersection jobsearch
head(links$href)
links$href <- gsub(x = links$href, pattern = "www.www.", replacement = "www.", fixed = TRUE)
#links <- check_for_button(links)
return(
list(
links = links, all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
while(iter_nr < max_iter){
iter_nr <- iter_nr + 1
target_link <- link_meta$links[1, ]
if(is.na(target_link$href)){
link_meta$links <- link_meta$links[-1, ]
next
}
link_meta <- parse_link(
target_link = target_link,
iter_nr = iter_nr, link_meta = link_meta,
use_selenium = FALSE, use_phantom = TRUE,
remDr = NULL, ses = ses
)
link_meta$links %>% head
grepl(link_meta$links$href, pattern = "career.centogene.com")
}
links <- link_meta$links
all_docs <- link_meta$all_docs
html_texts <- link_meta$html_texts
matches <- link_meta$matches
all_links <- link_meta$all_links
counts <- link_meta$counts
parsed_links <- link_meta$parsed_links
link <- links$href[1]
id <- c(iter_nr, target_link) %>% paste(collapse = "-")
print(link)
# workaround until frames are supported
if(is.na(link)){
exclude <- links$href %in% link
links <- links[!exclude, ]
message("skipping NA (frame link)")
return()
}
#all_docs[[id]] %>% showHtmlPage()
if(use_selenium){
out <- get_doc_selenium(url = link, remDr)
all_docs[[id]] <- out$doc
domain <- out$domain
}else if(use_phantom){
out <- get_doc_phantom(url = link, ses = ses)
ses <- out$ses
domain <- out$domain
all_docs[[id]] <- out$doc
}else{
all_docs[[id]] <- tryCatch(
expr = link %>% xml2::read_html(),
error = function(e) ""
)
}
has_doc <- nchar(all_docs[[id]] %>% toString)
if(has_doc){
all_links[[id]] <- all_docs[[id]] %>%
html_nodes(xpath = "//a") %>%
{data.frame(href = html_attr(x = ., name = "href"), text = html_text(.))}
# %>%{ifelse(test = substring(text = ., first = 1, last = 1) == "/", yes = paste0("https://www.", urltools::domain(link), .), no = .)}
html_texts[[id]] <- tryCatch(htm2txt::htm2txt(as.character(all_docs[[id]])),
error = function(e){
message(e)
warning(e)
return("")
})
}else{
all_links[[id]] <- data.frame(href = character(), text = character())
message("No content")
warning("No content")
return(
list(
links = links[-1, ], all_docs = all_docs, all_links = all_links,
parsed_links = parsed_links,
html_texts = html_texts, matches = matches, counts = counts
)
)
}
doc <- all_docs[[id]]
if(!exists("job_titles")) job_titles <- ""
matches[[iter_nr]] <- target_indicator_count(job_titles = job_titles, doc = doc)
counts[iter_nr] <- unlist(matches[[iter_nr]]) %>% as.numeric() %>% sum
# "https://careers.danone.com/de-global/" %in% links$href
# "https://careers.danone.com/de-global/" %in% links2$href
# todo: cant find src in code - maybe have to use swith to frame but cant use it as new link then
iframe_links_raw <- doc %>%
html_nodes(xpath = "//iframe") %>%
html_attr("src")
if(length(iframe_links_raw)){
if(is.na(iframe_links_raw)){
iframe_links_raw <- character()
}
}
iframe_links <- data.frame(href = iframe_links_raw, text = rep("iframe", length(iframe_links_raw)))
parsed_links[iter_nr, ]$href <- target_link$href
parsed_links[iter_nr, ]$text <- target_link$text
links <- rbind(iframe_links, links, all_links[[id]]) %>%
filter_links(domain = domain, parsed_links = parsed_links)
links <- links[!duplicated(links$href), ]
links$href
links <- rbind(parsed_links, links)
head(links)
exclude <- links$href %in% parsed_links$href
head(exclude)
links <- links[!exclude, ] %>% sort_links()
# taleo careersection jobsearch
head(links$href)
links$href <- gsub(x = links$href, pattern = "www.www.", replacement = "www.", fixed = TRUE)
ses$getUrl()
ses <- start_phantom()
ses$getUrl()
remove.packages("webdriver")
remove.packages("webdriver")
install.packages("webdriver")
install.packages("webdriver")
install.packages("webdriver", type = "source")
pjs <- webdriver::run_phantomjs()
pjs
webdriver::Session$new(port = pjs$port)
